"""ë…¸ì…˜ APIë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ëŸ¬"""

import re
import requests
from typing import Optional, Dict, Any, List
from urllib.parse import urlparse, parse_qs

class NotionAPICrawler:
    """ë…¸ì…˜ APIë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, notion_token: Optional[str] = None):
        """
        Args:
            notion_token: ë…¸ì…˜ API í† í° (ì„ íƒì‚¬í•­)
        """
        self.notion_token = notion_token
        self.session = requests.Session()
        
        if notion_token:
            self.session.headers.update({
                'Authorization': f'Bearer {notion_token}',
                'Content-Type': 'application/json',
                'Notion-Version': '2022-06-28'
            })
    
    def extract_page_id_from_url(self, notion_url: str) -> Optional[str]:
        """ë…¸ì…˜ URLì—ì„œ í˜ì´ì§€ IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            notion_url: ë…¸ì…˜ í˜ì´ì§€ URL
            
        Returns:
            í˜ì´ì§€ ID ë˜ëŠ” None
        """
        try:
            # URL íŒŒì‹±
            parsed_url = urlparse(notion_url)
            
            # ê²½ë¡œì—ì„œ í˜ì´ì§€ ID ì¶”ì¶œ
            path = parsed_url.path
            
            # ì¼ë°˜ì ì¸ ë…¸ì…˜ URL íŒ¨í„´ë“¤
            patterns = [
                r'/([a-f0-9]{32})',  # 32ìë¦¬ hex
                r'/([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})',  # UUID í˜•íƒœ
                r'/([a-f0-9]{8}[a-f0-9]{4}[a-f0-9]{4}[a-f0-9]{4}[a-f0-9]{12})',  # UUID without hyphens
            ]
            
            for pattern in patterns:
                match = re.search(pattern, path)
                if match:
                    page_id = match.group(1)
                    # í•˜ì´í”ˆ ì œê±°í•˜ê³  32ìë¦¬ë¡œ ë§ì¶¤
                    page_id = page_id.replace('-', '')
                    if len(page_id) == 32:
                        # UUID í˜•íƒœë¡œ ë³€í™˜
                        formatted_id = f"{page_id[:8]}-{page_id[8:12]}-{page_id[12:16]}-{page_id[16:20]}-{page_id[20:]}"
                        return formatted_id
            
            return None
            
        except Exception as e:
            print(f"í˜ì´ì§€ ID ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_page_content_public(self, notion_url: str) -> Optional[str]:
        """ê³µê°œ ë…¸ì…˜ í˜ì´ì§€ì˜ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (API í† í° ë¶ˆí•„ìš”).
        
        Args:
            notion_url: ë…¸ì…˜ í˜ì´ì§€ URL
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë˜ëŠ” None
        """
        try:
            print(f"ğŸ” ê³µê°œ ë…¸ì…˜ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„: {notion_url}")
            
            # ê³µê°œ í˜ì´ì§€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œì„ ì‹œë„
            response = self.session.get(notion_url, timeout=10)
            
            if response.status_code == 200:
                html_content = response.text
                
                # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')
                
                extracted_text = []
                
                # 1. ë‹¤ì–‘í•œ ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
                meta_tags = [
                    ('og:title', 'ì œëª©'),
                    ('og:description', 'ì„¤ëª…'),
                    ('description', 'í˜ì´ì§€ ì„¤ëª…'),
                    ('twitter:title', 'íŠ¸ìœ„í„° ì œëª©'),
                    ('twitter:description', 'íŠ¸ìœ„í„° ì„¤ëª…'),
                    ('apple-mobile-web-app-title', 'ì•± ì œëª©')
                ]
                
                for property_name, label in meta_tags:
                    meta = soup.find('meta', property=property_name) or soup.find('meta', attrs={'name': property_name})
                    if meta and meta.get('content'):
                        content = meta['content'].strip()
                        if content and content not in [item.split(': ', 1)[-1] for item in extracted_text]:
                            extracted_text.append(f"{label}: {content}")
                
                # 2. ì œëª© íƒœê·¸ ì¶”ì¶œ
                title_tag = soup.find('title')
                if title_tag:
                    title_text = title_tag.get_text().strip()
                    if title_text and "Notion" not in title_text and title_text not in [item.split(': ', 1)[-1] for item in extracted_text]:
                        extracted_text.append(f"í˜ì´ì§€ ì œëª©: {title_text}")
                
                # 3. JSON-LD êµ¬ì¡°í™” ë°ì´í„° ì¶”ì¶œ
                json_ld_scripts = soup.find_all('script', type='application/ld+json')
                for script in json_ld_scripts:
                    try:
                        import json
                        data = json.loads(script.string)
                        if isinstance(data, dict):
                            if 'name' in data:
                                extracted_text.append(f"êµ¬ì¡°í™” ë°ì´í„° ì´ë¦„: {data['name']}")
                            if 'description' in data:
                                extracted_text.append(f"êµ¬ì¡°í™” ë°ì´í„° ì„¤ëª…: {data['description']}")
                            if 'headline' in data:
                                extracted_text.append(f"í—¤ë“œë¼ì¸: {data['headline']}")
                    except:
                        continue
                
                # 4. ë…¸ì…˜ íŠ¹í™” ë°ì´í„° ì¶”ì¶œ ì‹œë„
                # ë…¸ì…˜ í˜ì´ì§€ì˜ ì´ˆê¸° ìƒíƒœ ë°ì´í„°ë¥¼ ì°¾ì•„ë³´ê¸°
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string and ('window.__INITIAL_STATE__' in script.string or 'window.__NEXT_DATA__' in script.string):
                        script_content = script.string
                        
                        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì œëª©ì´ë‚˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                        import re
                        
                        # í•œê¸€ í…ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸°
                        korean_patterns = [
                            r'"title":\s*"([^"]*[ê°€-í£][^"]*)"',
                            r'"name":\s*"([^"]*[ê°€-í£][^"]*)"',
                            r'"text":\s*"([^"]*[ê°€-í£][^"]*)"',
                            r'"content":\s*"([^"]*[ê°€-í£][^"]*)"'
                        ]
                        
                        for pattern in korean_patterns:
                            matches = re.findall(pattern, script_content)
                            for match in matches[:5]:  # ìƒìœ„ 5ê°œë§Œ
                                if len(match) > 2 and match not in [item.split(': ', 1)[-1] for item in extracted_text]:
                                    extracted_text.append(f"ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„°: {match}")
                        
                        break
                
                # 5. ê¸°ë³¸ í…ìŠ¤íŠ¸ ë…¸ë“œì—ì„œ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ì°¾ê¸°
                text_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'span', 'div'])
                for element in text_elements[:20]:  # ìƒìœ„ 20ê°œë§Œ í™•ì¸
                    text = element.get_text().strip()
                    if text and len(text) > 5 and len(text) < 200:  # ì ì ˆí•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸
                        # í•œê¸€ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ìš°ì„ 
                        if re.search(r'[ê°€-í£]', text) and text not in [item.split(': ', 1)[-1] for item in extracted_text]:
                            extracted_text.append(f"í˜ì´ì§€ í…ìŠ¤íŠ¸: {text}")
                            if len(extracted_text) > 10:  # ë„ˆë¬´ ë§ìœ¼ë©´ ì¤‘ë‹¨
                                break
                
                if extracted_text:
                    result = '\n'.join(extracted_text)
                    print(f"âœ… ê³µê°œ í˜ì´ì§€ì—ì„œ í™•ì¥ëœ ì •ë³´ ì¶”ì¶œ: {len(result)} ë¬¸ì")
                    print(f"ì¶”ì¶œëœ í•­ëª© ìˆ˜: {len(extracted_text)}ê°œ")
                    return result
                else:
                    print("âŒ ê³µê°œ í˜ì´ì§€ì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return None
            else:
                print(f"âŒ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ ê³µê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def get_page_content_with_api(self, page_id: str) -> Optional[str]:
        """ë…¸ì…˜ APIë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            page_id: ë…¸ì…˜ í˜ì´ì§€ ID
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë˜ëŠ” None
        """
        if not self.notion_token:
            print("âŒ ë…¸ì…˜ API í† í°ì´ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        try:
            print(f"ğŸ” ë…¸ì…˜ APIë¡œ í˜ì´ì§€ ì¡°íšŒ: {page_id}")
            
            # í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            page_url = f"https://api.notion.com/v1/pages/{page_id}"
            page_response = self.session.get(page_url)
            
            if page_response.status_code != 200:
                print(f"âŒ í˜ì´ì§€ ì¡°íšŒ ì‹¤íŒ¨: {page_response.status_code}")
                return None
            
            page_data = page_response.json()
            
            # ë¸”ë¡ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
            blocks_url = f"https://api.notion.com/v1/blocks/{page_id}/children"
            blocks_response = self.session.get(blocks_url)
            
            if blocks_response.status_code != 200:
                print(f"âŒ ë¸”ë¡ ì¡°íšŒ ì‹¤íŒ¨: {blocks_response.status_code}")
                return None
            
            blocks_data = blocks_response.json()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_texts = []
            
            # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
            if 'properties' in page_data:
                title_prop = page_data['properties'].get('title') or page_data['properties'].get('Name')
                if title_prop and 'title' in title_prop:
                    title_texts = [item.get('text', {}).get('content', '') for item in title_prop['title']]
                    if title_texts:
                        extracted_texts.append(f"ì œëª©: {''.join(title_texts)}")
            
            # ë¸”ë¡ ì½˜í…ì¸  ì¶”ì¶œ
            for block in blocks_data.get('results', []):
                text = self._extract_text_from_block(block)
                if text:
                    extracted_texts.append(text)
            
            if extracted_texts:
                result = '\n'.join(extracted_texts)
                print(f"âœ… ë…¸ì…˜ APIë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì„±ê³µ: {len(result)} ë¬¸ì")
                return result
            else:
                print("âŒ ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None
                
        except Exception as e:
            print(f"âŒ ë…¸ì…˜ API í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _extract_text_from_block(self, block: Dict[str, Any]) -> Optional[str]:
        """ë…¸ì…˜ ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            block_type = block.get('type')
            
            if not block_type:
                return None
            
            block_data = block.get(block_type, {})
            
            # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë¸”ë¡ íƒ€ì…ë“¤
            text_types = ['paragraph', 'heading_1', 'heading_2', 'heading_3', 
                         'bulleted_list_item', 'numbered_list_item', 'quote', 'callout']
            
            if block_type in text_types:
                rich_text = block_data.get('rich_text', [])
                if rich_text:
                    texts = [item.get('text', {}).get('content', '') for item in rich_text]
                    return ''.join(texts)
            
            return None
            
        except Exception:
            return None
    
    async def crawl_notion_page(self, notion_url: str) -> Optional[str]:
        """ë…¸ì…˜ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            notion_url: ë…¸ì…˜ í˜ì´ì§€ URL
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë˜ëŠ” None
        """
        try:
            print(f"ğŸ” ë…¸ì…˜ API í¬ë¡¤ëŸ¬ë¡œ í˜ì´ì§€ ë¶„ì„: {notion_url}")
            
            # 1ìˆœìœ„: ê³µê°œ í˜ì´ì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì‹œë„
            public_content = self.get_page_content_public(notion_url)
            if public_content:
                return public_content
            
            # 2ìˆœìœ„: API í† í°ì´ ìˆìœ¼ë©´ API ì‚¬ìš©
            if self.notion_token:
                page_id = self.extract_page_id_from_url(notion_url)
                if page_id:
                    api_content = self.get_page_content_with_api(page_id)
                    if api_content:
                        return api_content
            
            # 3ìˆœìœ„: ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ì œê³µ
            print("âš ï¸ ë…¸ì…˜ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ ì •ë³´ ì œê³µ")
            
            # URLì—ì„œ í˜ì´ì§€ ID ì¶”ì¶œí•˜ì—¬ ê¸°ë³¸ ì •ë³´ ìƒì„±
            page_id = self.extract_page_id_from_url(notion_url)
            
            fallback_info = []
            fallback_info.append(f"ë…¸ì…˜ í˜ì´ì§€ ë§í¬: {notion_url}")
            
            if page_id:
                fallback_info.append(f"í˜ì´ì§€ ID: {page_id}")
            
            # URL êµ¬ì¡° ë¶„ì„
            if 'notion.site' in notion_url:
                fallback_info.append("ìœ í˜•: ë…¸ì…˜ ê³µê°œ ì‚¬ì´íŠ¸")
            elif 'notion.so' in notion_url:
                fallback_info.append("ìœ í˜•: ë…¸ì…˜ ì›Œí¬ìŠ¤í˜ì´ìŠ¤")
            
            fallback_info.append("ìƒíƒœ: JavaScript ë Œë”ë§ í•„ìš” - ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í™•ì¸ í•„ìš”")
            fallback_info.append("ì°¸ê³ : ì‹¤ì œ ì½˜í…ì¸ ëŠ” ë…¸ì…˜ ì•±ì´ë‚˜ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸í•˜ì„¸ìš”")
            
            result = '\n'.join(fallback_info)
            print(f"âœ… ê¸°ë³¸ ì •ë³´ ì œê³µ: {len(result)} ë¬¸ì")
            return result
            
        except Exception as e:
            print(f"âŒ ë…¸ì…˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_notion_api_crawler():
    """ë…¸ì…˜ API í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    crawler = NotionAPICrawler()
    
    # https://sequoia-ray-2fa.notion.site/1f732f6a257980a0a95ad7126bc0e46e?pvs=4
    test_url = "https://sequoia-ray-2fa.notion.site/1f732f6a257980a0a95ad7126bc0e46e?pvs=4"

    result = await crawler.crawl_notion_page(test_url)
    
    if result:
        print(f"í¬ë¡¤ë§ ê²°ê³¼:\n{result}")
    else:
        print("í¬ë¡¤ë§ ì‹¤íŒ¨")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_notion_api_crawler()) 