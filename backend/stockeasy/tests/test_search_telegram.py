"""
ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

ì´ ëª¨ë“ˆì€ Stockeasy ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""
import hashlib
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
import re

from zoneinfo import ZoneInfo

from common.services.agent_llm import get_agent_llm

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ ëª¨ë“ˆì„ importí•  ìˆ˜ ìˆê²Œ í•¨


from uuid import UUID

from common.services.embedding_models import EmbeddingModelType
from common.models.token_usage import ProjectType
from common.services.retrievers.contextual_bm25 import ContextualBM25Config
from common.services.retrievers.hybrid import HybridRetriever, HybridRetrieverConfig
from common.services.retrievers.models import RetrievalResult
from common.services.retrievers.semantic import SemanticRetriever, SemanticRetrieverConfig
from common.services.vector_store_manager import VectorStoreManager

import asyncio
import json
from loguru import logger
from datetime import datetime, timedelta, timezone
import pytz
from typing import Dict, Any, List, Optional, Set, Union
from common.app import LoadEnvGlobal

LoadEnvGlobal()
# LangSmith í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
#os.environ["LANGCHAIN_TRACING"] = "true"
#os.environ["LANGCHAIN_TRACING_V2"] = "true"
#os.environ["LANGCHAIN_PROJECT"] = "stockeasy_multiagent"
# LANGSMITH_API_KEYëŠ” .env íŒŒì¼ì—ì„œ ë¡œë“œë¨

from stockeasy.models.agent_io import QuestionAnalysisResult, RetrievedTelegramMessage
from common.core.config import settings
# ë¡œê·¸ ì‹œê°„ì„ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
logger.remove()  # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# í•œêµ­ ì‹œê°„ìœ¼ë¡œ ì„¤ì •ëœ ë¡œê±° ì¶”ê°€ (ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •)
logger.add(
    sys.stdout,
    format="{time:YYYY-MM-DD HH:mm:ss} KST | {level: <8} | {name}:{line} - {message}",
    level="INFO",
    colorize=True,
)


# ë¡œê±° ì„¤ì • ì´í›„ì— ëª¨ë“ˆ ì„í¬íŠ¸
from common.core.database import get_db_session
def _calculate_time_weight(created_at: datetime) -> float:
        """
        ë©”ì‹œì§€ ìƒì„± ì‹œê°„ ê¸°ë°˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            created_at: ë©”ì‹œì§€ ìƒì„± ì‹œê°„ (datetime ê°ì²´)
            
        Returns:
            ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.4 ~ 1.0)
        """
        try:
            seoul_tz = timezone(timedelta(hours=9), 'Asia/Seoul')
        
            # naive datetimeì¸ ê²½ìš° ì„œë²„ ë¡œì»¬ ì‹œê°„(Asia/Seoul)ìœ¼ë¡œ ê°„ì£¼
            if created_at.tzinfo is None:
                created_at = created_at.replace(tzinfo=ZoneInfo("Asia/Seoul"))
                
            # nowë„ timezone ì •ë³´ë¥¼ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •
            now = datetime.now(seoul_tz)
            delta = now - created_at
            
            # ì‹œê°„ ì°¨ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì„¤ì •
            if delta.days < 1:  # 24ì‹œê°„ ì´ë‚´
                return 1.0
            elif delta.days < 7:  # 1ì£¼ì¼ ì´ë‚´
                return 0.9
            elif delta.days < 14:  # 2ì£¼ì¼ ì´ë‚´
                return 0.8
            elif delta.days < 30:  # 1ê°œì›” ì´ë‚´
                return 0.6
            else:  # 1ê°œì›” ì´ìƒ
                return 0.4
                
        except Exception as e:
            logger.warning(f"ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5  # ì˜¤ë¥˜ ì‹œ ì¤‘ê°„ê°’ ë°˜í™˜
        
def _calculate_message_importance( message: str) -> float:
        """
        ë©”ì‹œì§€ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            message: ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•  ë©”ì‹œì§€
            
        Returns:
            0~1 ì‚¬ì´ì˜ ì¤‘ìš”ë„ ì ìˆ˜
        """
        importance_score = 0.0
        
        # 1. ê¸ˆì•¡/ìˆ˜ì¹˜ ì •ë³´ í¬í•¨ ì—¬ë¶€ (40%)
        if re.search(r'[0-9]+(?:,[0-9]+)*(?:\.[0-9]+)?%?ì›?', message):
            importance_score += 0.4
            
        # 2. ì£¼ìš” í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ (40%)
        important_keywords = [
            'ì‹¤ì ', 'ê³µì‹œ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ',
            'ê³„ì•½', 'íŠ¹í—ˆ', 'ì¸ìˆ˜', 'í•©ë³‘', 'M&A',
            'ìƒí•œê°€', 'í•˜í•œê°€', 'ê¸‰ë“±', 'ê¸‰ë½',
            'ëª©í‘œê°€', 'íˆ¬ìì˜ê²¬', 'ë¦¬í¬íŠ¸'
        ]
        keyword_count = sum(1 for keyword in important_keywords if keyword in message)
        if keyword_count > 0:
            importance_score += min(0.4, keyword_count * 0.2)  # í‚¤ì›Œë“œë‹¹ 0.2ì , ìµœëŒ€ 0.4ì 
        
        # 3. ë©”ì‹œì§€ ê¸¸ì´ ê°€ì¤‘ì¹˜ (20%)
        msg_length = len(message)
        if 50 <= msg_length <= 500:
            importance_score += 0.2
        elif 20 <= msg_length < 50 or 500 < msg_length <= 1000:
            importance_score += 0.1
            
        return importance_score

def _get_message_hash( content: str) -> str:
        """
        ë©”ì‹œì§€ ë‚´ìš©ì˜ í•´ì‹œê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            content: ë©”ì‹œì§€ ë‚´ìš©
            
        Returns:
            ë©”ì‹œì§€ í•´ì‹œê°’
        """
        # ë©”ì‹œì§€ ì „ì²˜ë¦¬ (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)
        normalized_content = re.sub(r'\s+', ' ', content).strip().lower()
        
        # ë„ˆë¬´ ê¸´ ë©”ì‹œì§€ëŠ” ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        if len(normalized_content) > 200:
            normalized_content = normalized_content[:200]
            
        # SHA-256 í•´ì‹œ ìƒì„±í•˜ì—¬ ë°˜í™˜
        return hashlib.sha256(normalized_content.encode('utf-8')).hexdigest() 

def _is_duplicate(message: str, seen_messages: Set[str]) -> bool:
        """
        ë©”ì‹œì§€ê°€ ì´ë¯¸ ì²˜ë¦¬ëœ ë©”ì‹œì§€ ì¤‘ ì¤‘ë³µì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        Args:
            message: ê²€ì‚¬í•  ë©”ì‹œì§€
            seen_messages: ì´ë¯¸ ì²˜ë¦¬ëœ ë©”ì‹œì§€ í•´ì‹œ ì§‘í•©
            
        Returns:
            ì¤‘ë³µ ì—¬ë¶€
        """
        # ë©”ì‹œì§€ í•´ì‹œ ìƒì„±
        message_hash = _get_message_hash(message)
        
        # ì¤‘ë³µ í™•ì¸
        if message_hash in seen_messages:
            return True
            
        return False

async def _search_messages(search_query: str, k: int, threshold: float, user_id: Optional[Union[str, UUID]] = None, search_type:str = "hybrid") -> List[RetrievedTelegramMessage]:
        """
        í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            search_query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë©”ì‹œì§€ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            user_id: ì‚¬ìš©ì ID (ë¬¸ìì—´ ë˜ëŠ” UUID ê°ì²´)
            
        Returns:
            ê²€ìƒ‰ëœ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ëª©ë¡
        """
        try:
            logger.info(f"Generated search query: {search_query}")
            
            # ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ë²¡í„° ìƒì„±
            
            # ì´ˆê¸° ê²€ìƒ‰ì€ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ í›„ í•„í„°ë§
            initial_k = min(k * 3, 30)  # ì ì–´ë„ ì›í•˜ëŠ” kì˜ 3ë°°, ìµœëŒ€ 30ê°œê¹Œì§€
            
            # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
            vs_manager = VectorStoreManager(
                embedding_model_type=EmbeddingModelType.OPENAI_3_LARGE,
                project_name="stockeasy",
                namespace=settings.PINECONE_NAMESPACE_STOCKEASY_TELEGRAM
            )

            # UUID ë³€í™˜ ë¡œì§: ë¬¸ìì—´ì´ë©´ UUIDë¡œ ë³€í™˜, UUID ê°ì²´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, Noneì´ë©´ None
            if user_id != "test_user":
                parsed_user_id = UUID(user_id) if isinstance(user_id, str) else user_id
            else:
                parsed_user_id = None

            semantic_retriever_config = SemanticRetrieverConfig(min_score=threshold,
                                               user_id=parsed_user_id,
                                               project_type=ProjectType.STOCKEASY    )
            # ì‹œë§¨í‹± ê²€ìƒ‰ ì„¤ì •
            semantic_retriever = SemanticRetriever(
                config=semantic_retriever_config,
                vs_manager=vs_manager
            )
            
            if search_type == "hybrid":
                hybrid_retriever = HybridRetriever(
                    config=HybridRetrieverConfig(
                        semantic_config=semantic_retriever_config,
                        contextual_bm25_config=ContextualBM25Config(
                                            min_score=0.1,
                                            bm25_weight=0.6,
                                            context_weight=0.4,
                                            context_window_size=3
                                        ),
                        semantic_weight=0.6,
                        contextual_bm25_weight=0.4,
                        vector_multiplier=2
                    ),
                    vs_manager=vs_manager
                )
                #result:RetrievalResult = await hybrid_retriever.retrieve_vector_then_bm25(
                result:RetrievalResult = await hybrid_retriever.retrieve_vector_then_rerank(
                        query=search_query, 
                        top_k=initial_k
                    )
            else:     
                result:RetrievalResult = await semantic_retriever.retrieve(
                    query=search_query, 
                    top_k=initial_k,#k * 2,
                )
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            # result: RetrievalResult = await semantic_retriever.retrieve(
            #     query=search_query, 
            #     top_k=initial_k,#k * 2,
            # )
            
            if len(result.documents) == 0:
                logger.warning(f"No telegram messages found for query: {search_query}")
                return []
                
            logger.info(f"Found {len(result.documents)} telegram messages")
            
            # ì¤‘ë³µ ë©”ì‹œì§€ í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°
            processed_messages = []
            seen_messages = set()  # ì¤‘ë³µ í™•ì¸ìš©
            temp_docs = []
            
            for doc in result.documents:
                doc_metadata = doc.metadata
                content = doc.page_content# doc_metadata.get("text", "")
                
                # ë‚´ìš©ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ë©”ì‹œì§€ ì œì™¸
                if not content or len(content) < 20:
                    continue
                
                normalized_content = re.sub(r'\s+', ' ', content).strip().lower()
                # ì¤‘ë³µ ë©”ì‹œì§€ í™•ì¸
                if _is_duplicate(normalized_content, seen_messages):
                    logger.info(f"ì¤‘ë³µ ë©”ì‹œì§€ ì œì™¸: {normalized_content[:50]}")
                    continue
                    
                seen_messages.add(_get_message_hash(normalized_content))
                temp_docs.append(doc)
            # ì¤‘ë³µ ì œê±°ëœ ì²­í¬ë¡œ. ë¦¬ë­í‚¹ ìˆ˜í–‰

            # ì¢…ë³µ ì œê±°ëœ ê²ƒìœ¼ë¡œ
            for doc in temp_docs:
                doc_metadata = doc.metadata
                content = doc.page_content
                # ë©”ì‹œì§€ ì¤‘ìš”ë„ ê³„ì‚°
                importance_score = _calculate_message_importance(content)
                
                # ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
                message_created_at_data = doc.metadata["message_created_at"]
                message_created_at = None
                
                # message_created_atì„ datetime ê°ì²´ë¡œ ë³€í™˜ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
                if isinstance(message_created_at_data, str):
                    # ISO í˜•ì‹ ë¬¸ìì—´ì¸ ê²½ìš°
                    try:
                        message_created_at = datetime.fromisoformat(message_created_at_data)
                    except (ValueError, TypeError):
                        # ISO í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                        print(f"ISO í˜•ì‹ì´ ì•„ë‹Œ ë¬¸ìì—´: {message_created_at_data}, ë‹¤ë¥¸ í˜•ì‹ ì‹œë„")
                        try:
                            # ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                            message_created_at = datetime.fromtimestamp(float(message_created_at_data))
                        except (ValueError, TypeError):
                            # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                            print(f"ì‹œê°„ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {message_created_at_data}, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
                            message_created_at = datetime.now()
                elif isinstance(message_created_at_data, (int, float)):
                    # ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ì¸ ê²½ìš°
                    try:
                        message_created_at = datetime.fromtimestamp(float(message_created_at_data))
                    except (ValueError, TypeError):
                        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                        print(f"íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì‹¤íŒ¨: {message_created_at_data}, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
                        message_created_at = datetime.now()
                else:
                    # ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì¸ ê²½ìš° í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                    print(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‹œê°„ í˜•ì‹: {type(message_created_at_data)}, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
                    message_created_at = datetime.now()
                
                time_weight = _calculate_time_weight(message_created_at)
                
                # ìµœì¢… ì ìˆ˜ = ìœ ì‚¬ë„ * ì¤‘ìš”ë„ * ì‹œê°„ ê°€ì¤‘ì¹˜
                #final_score = doc.score * importance_score * time_weight
                final_score = (doc.score * 0.5) + (importance_score * 0.3) + (time_weight * 0.2)
                # ë©”ì‹œì§€ ë°ì´í„° êµ¬ì„±
                message:RetrievedTelegramMessage = {
                    "content": content,
                    #"channel_name": doc_metadata.get("channel_title", "ì•Œ ìˆ˜ ì—†ìŒ"), # ê·¸ëŸ¬ë‚˜ ìˆ¨ê²¨ì•¼í•¨
                    "message_created_at": message_created_at,
                    "final_score": final_score,
                    "metadata": doc_metadata
                }
                
                processed_messages.append(message)
            
            # ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ kê°œ ì„ íƒ
            processed_messages.sort(key=lambda x: x["final_score"], reverse=True)
            logger.info(f"ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ ë©”ì‹œì§€ ìˆ˜: {len(processed_messages)}")
            result_messages = processed_messages[:k]
            
            # ì ìˆ˜ ë¶„í¬ ì •ê·œí™”
            if result_messages:
                max_score = max(msg["final_score"] for msg in result_messages)
                min_score = min(msg["final_score"] for msg in result_messages)
                score_range = max_score - min_score if max_score > min_score else 1.0
                
                for msg in result_messages:
                    msg["normalized_score"] = (msg["final_score"] - min_score) / score_range
            
            return result_messages
            
        except Exception as e:
            logger.exception(f"Error searching telegram messages: {str(e)}")
            raise 

async def main():
#     _search_query = """ì‚¬ìš©ì ì§ˆë¬¸: ë©”ëª¨ë¦¬ ê°€ê²© ì „ë§ê³¼ ì•ìœ¼ë¡œ ì‹¤ì ì€?
# ì¢…ëª©ëª…: ì‚¼ì„±ì „ì
# ì¢…ëª©ì½”ë“œ: 005930
# """
    #_search_query = "ê²Œì„ ì¸ì¡°ì´ì— ê´€í•´ì„œ ì„¤ëª…í•´ì¤˜. ìµœê·¼ ì¶œì‹œí–ˆëŠ”ë° ë°˜ì‘ì´ ì–´ë–¤ í¸ì´ì§€?"
    _search_query = "ê²Œì„ ì¸ì¡°ì´ì˜ ë°˜ì‘ì— ê´€í•´ì„œ ì•Œë ¤ì£¼ê³ , ì ‘ì†ì ì¶”ì´ë¥¼ ì •ë¦¬í•´ë´"
    #_search_query = "dram ì»¤ë¯¸ë””í‹° ê°€ê²©ì´ ìƒìŠ¹ì„¸ì¸ë°, ê·¸ ì›ì¸ê³¼ ì‚¼ì„±ì „ìì˜ ì£¼ê°€ì˜ ê´€ê³„ëŠ”?"
    #_search_query = "ë§ˆì´í¬ë¡œì»¨í…ì†” ì‹¤ì ê³¼ ì˜¬í•´ ì „ë§ì€?"
    _k = 15
    _threshold = 0.22
    #_user_id = "blueslame@gmail.com"

    result:List[RetrievedTelegramMessage] = await _search_messages(search_query=_search_query, 
                                                                   k=_k, threshold=_threshold, 
                                                                   #search_type="semantic"
                                                                   )
    # ê²°ê³¼ ì˜ˆì˜ê²Œ ì¶œë ¥
    print("\n" + "="*80)
    print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{_search_query}' (ì„ê³„ê°’: {_threshold}, ê²°ê³¼ ìˆ˜: {len(result)})")
    print("="*80)
    docs = []
    for idx, item in enumerate(result[:4]):
        rerank_score = item.get("metadata", {}).get("rerank_score", 0.0)
        print(f"\nğŸ“ ê²°ê³¼ #{idx} [ìœ ì‚¬ë„: {item.get('final_score', 0):.4f}, ë¦¬ë­í‚¹ ì ìˆ˜: {rerank_score:.4f}]")
        print(f"ğŸ“… ë‚ ì§œ: {item.get('message_created_at')}")
        print("-"*60)
        print(f"{item.get('content', 'ë‚´ìš© ì—†ìŒ')[:200]}...")
        print("-"*60)
        text = f"ë‚ ì§œ: {item.get('message_created_at')}\në‚´ìš©: {item.get('content', 'ë‚´ìš© ì—†ìŒ')}"
        text += "------------------"
        docs.append(text)
    
    print("\n" + "="*80)
    print(f"ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(result)}ê°œ ê²°ê³¼")
    print("="*80)

    # f-stringì—ì„œ ë°±ìŠ¬ë˜ì‹œ ë¬¸ì œ í•´ê²°
    prompt = (
        f"ì •í•´ì§„ ë¬¸ì„œì˜ ë‚´ìš© ì•ˆì—ì„œë§Œ ë‹µí•´ì£¼ì„¸ìš”. ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì£¼ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {_search_query}\n\n"
        f"ë‚´ìš© : {chr(10).join(docs)}"
    )
    llm = get_agent_llm("test_agent")
    response = await llm.ainvoke_with_fallback(prompt)
    print("-"*80)
    print(f"## ì§ˆë¬¸ : {_search_query}")
    print("-"*80)
    print(f"##ë‹µë³€ : \n{response.content}")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(main()) 