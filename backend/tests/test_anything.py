import asyncio
from datetime import datetime
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)

from common.app import LoadEnvGlobal
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
LoadEnvGlobal()
from common.services.embedding_models import EmbeddingModelType
from common.services.embedding import EmbeddingService

from common.services.llm_models import LLMModels
import vertexai
from vertexai.language_models import TextEmbeddingModel
from loguru import logger

from google.oauth2 import service_account

from openai import OpenAI
from common.core.config import settings

from langchain_openai import OpenAIEmbeddings

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_vertexai.embeddings import VertexAIEmbeddings

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from rich.console import Console





# ë¡œê¹… ì„¤ì •
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

async def test_google_storage_service():
    from common.services.storage import GoogleCloudStorageService
    
    storage = GoogleCloudStorageService(
        project_id=settings.GOOGLE_CLOUD_PROJECT,
        bucket_name=settings.GOOGLE_CLOUD_STORAGE_BUCKET_STOCKEASY,
        credentials_path=settings.GOOGLE_APPLICATION_CREDENTIALS
    )
    folder_path = Path("telegram_files/2025-02-28")
    
    # í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ìˆœíšŒ
    for file_path in folder_path.iterdir():
        if file_path.is_file():  # íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path.name}")
            
            # ì—¬ê¸°ì— ê° íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
            # ì˜ˆ: íŒŒì¼ ì—…ë¡œë“œ, íŒŒì¼ ì²˜ë¦¬ ë“±
            
            # ì˜ˆì‹œ: íŒŒì¼ ì •ë³´ ì¶œë ¥
            file_size = file_path.stat().st_size
            print(f"  - íŒŒì¼ í¬ê¸°: {file_size} ë°”ì´íŠ¸")
            
            # ë¹„ë™ê¸° ì‘ì—…ì´ í•„ìš”í•œ ê²½ìš° await ì‚¬ìš©
            # file_path = "telegram_files/2025-02-28/SKì´í„°ë‹‰ìŠ¤ï¼»475150ï¼½ë§¤ë ¥ì ì¸_ì‹ ì„±ì¥_ë™ë ¥_í™•ë³´_20250228_Kiwoom_982560.pdf"
            target_path = "Stockeasy/collected_auto/íƒ¤ë˜ê·¸ë¨/dev/ê³µì‹/"
            target_full_path = target_path + file_path.name
            await storage.upload_from_filename(target_full_path, file_path)

    
    #ss = await storage.get_download_url("AMD AI ì»¨í¼ëŸ°ìŠ¤_2023.12.06.docx")
    #print(ss)
    #storage.upload_file("test3.txt", "Hello, World!3333333")

def test_google_storage():
    # ì¸ì¦ ì„¤ì •
    from google.cloud import storage
    from google.oauth2 import service_account
    credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_VERTEXAI", "")
    credentials = service_account.Credentials.from_service_account_file(credentials_path)

    # ìŠ¤í† ë¦¬ì§€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    storage_client = storage.Client(credentials=credentials)
    buckets = list(storage_client.list_buckets())
    print("Storage Buckets:", [bucket.name for bucket in buckets])
    # ë²„í‚· ì ‘ê·¼
    print(f"bucket: {settings.GOOGLE_CLOUD_STORAGE_BUCKET_DOCEASY}")
    bucket = storage_client.bucket(settings.GOOGLE_CLOUD_STORAGE_BUCKET_DOCEASY)
    # # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ
    # blob = bucket.blob('test.txt')
    # blob.upload_from_string('Hello, World!')

    # ê¸°ì¡´ íŒŒì´ ì—…ë¡œê·¸
    # file_path = 'D:/Work/ë‹¥ì´ì§€_í…ŒìŠ¤íŠ¸ë¬¸ì„œ/í™”ì¥í’ˆ_3QPre_ê²¬ì¡°í•œ_ì—…í™©,_ì‹¤ì ì€_ê¸°ëŒ€ì¹˜_ë¶€í•©_ì˜ˆìƒ.pdf'# íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ê²½ë¡œ ì œì™¸)
    # file_name = os.path.basename(file_path)
    
    # # ìŠ¤í† ë¦¬ì§€ì— ì—…ë¡œë“œ (documents í´ë” ì•„ë˜ì— ì €ì¥)
    # blob = bucket.blob(f'doc/{file_name}')
    # blob.upload_from_filename(file_path)
    
    # print(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file_name}")
    blobs = bucket.list_blobs()
    print("\n=== ì „ì²´ íŒŒì¼ ëª©ë¡ ===")
    for blob in blobs:
        print(f"- {blob.name} (í¬ê¸°: {blob.size} bytes)")


    #2. íŠ¹ì • í´ë” ë‚´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
    prefix = "doc/"  # í´ë” ê²½ë¡œ
    delimiter = "/"        # í´ë” êµ¬ë¶„ì
    blobs = bucket.list_blobs(prefix=prefix, delimiter=delimiter)
    
    print(f"\n=== {prefix} í´ë” ë‚´ íŒŒì¼ ëª©ë¡ ===")
    for blob in blobs:
        print(f"- {blob.name}")
    
    # 3. í´ë” ëª©ë¡ (prefixë¡œ ì‹œì‘í•˜ëŠ” í•˜ìœ„ í´ë”ë“¤)
    print(f"\n=== {prefix} í•˜ìœ„ í´ë” ëª©ë¡ ===")
    for prefix in blobs.prefixes:
        print(f"- {prefix}")

    blobs = bucket.list_blobs()
    print("\n=== ìƒì„¸ íŒŒì¼ ì •ë³´ ===")
    for blob in blobs:
        print(f"""
íŒŒì¼ëª…: {blob.name}
í¬ê¸°: {blob.size:,} bytes
ìƒì„±ì¼: {blob.time_created}
ìˆ˜ì •ì¼: {blob.updated}
Content Type: {blob.content_type}
ë‹¤ìš´ë¡œë“œ URL: {blob.public_url if blob.public_url else 'ë¹„ê³µê°œ'}
-------------------""")

def test_vertex_embedding():
    # Vertex AI ì´ˆê¸°í™”
    project_id = os.getenv("GOOGLE_PROJECT_ID_VERTEXAI")
    location = os.getenv("GOOGLE_LOCATION_VERTEXAI", "asia-northeast3")
    credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_VERTEXAI", "")
    if not project_id:
        raise ValueError("GOOGLE_PROJECT_ID_VERTEXAI í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print(f'project_id: {project_id}')
    print(f'location: {location}')
    print(f'credentials_path: {credentials_path}')
    # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ JSON íŒŒì¼ë¡œë¶€í„° credentials ê°ì²´ ìƒì„±
    credentials = service_account.Credentials.from_service_account_file(credentials_path)
    

    vertexai.init(project=project_id, location=location, credentials=credentials)
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    #model = TextEmbeddingModel.from_pretrained("textembedding-gecko@latest")
    model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")
    #model = TextEmbeddingModel.from_pretrained("text-embedding-004")
    
    # í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸
    texts = [
        "DeepSeek ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì•„ì¡Œë‹¤ê³  ìµœê·¼ ê¸°ì‚¬ì— ë§ì´ ë‚˜ì˜¤ë„¤. ì–¼ë§ˆë‚˜ ì¢‹ì•„ì¡Œë‹ˆ?",
    ]
    
    try:
        # ì„ë² ë”© ìƒì„±
        embeddings = model.get_embeddings(texts)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, embedding in enumerate(embeddings):
            #logger.info(f"Text {i+1}: {texts[i]}")
            #logger.info(f"Embedding dimension: {len(embedding.values)}")
            logger.info(f"First 5 values: {embedding.values[:5]}")
            logger.info("-" * 50)
            
        return True
        
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise

def test_kakao_embedding():
    """KF-Deberta ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© í…ŒìŠ¤íŠ¸"""
    try:
        from transformers import AutoModel, AutoTokenizer
        import torch
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        model = AutoModel.from_pretrained(settings.KAKAO_EMBEDDING_MODEL_PATH)
        tokenizer = AutoTokenizer.from_pretrained(settings.KAKAO_EMBEDDING_MODEL_PATH)
        

        # í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸
        texts = [
            "ì•ˆë…•í•˜ì„¸ìš”, í•œêµ­ì–´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
            "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
            "KF-DebertaëŠ” í•œêµ­ì–´ì— íŠ¹í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤.",
            "ì´ ëª¨ë¸ì€ í•œêµ­ì–´ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì˜ íŒŒì•…í•©ë‹ˆë‹¤."
        ]
        
        logger.info("KF-Deberta ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        #text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer)
        for i, text in enumerate(texts):
            # í† í°í™”
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            tokens = tokenizer.tokenize(text)
            other_tokens = tokenizer.encode(text)
            logger.info(f"í† í°í™” ê²°ê³¼: {tokens}, í† í°ìˆ˜: {len(tokens)}")
            logger.info(f"í† í°í™” ê²°ê³¼: {other_tokens}, í† í°ìˆ˜: {len(other_tokens)}")
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                outputs = model(**inputs)
            
            # [CLS] í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ (ë¬¸ì¥ ì „ì²´ í‘œí˜„)
            sentence_embedding = outputs.last_hidden_state[0, 0, :].numpy()
            
            logger.info(f"\nText {i+1}: {text}")
            #logger.info(f"ì„ë² ë”© ì°¨ì›: {len(sentence_embedding)}")
            #logger.info(f"ì²˜ìŒ 5ê°œ ê°’: {sentence_embedding[:5]}")
            logger.info("-" * 50)
        
        return True
        
    except Exception as e:
        logger.error(f"KF-Deberta ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise
def test_kakao_gen():
    from transformers import AutoModel, AutoTokenizer
    import torch
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model = AutoModel.from_pretrained(settings.KAKAO_EMBEDDING_MODEL_PATH)
    tokenizer = AutoTokenizer.from_pretrained(settings.KAKAO_EMBEDDING_MODEL_PATH)
        
    # ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
    text = "ì•ˆë…•"
    
    # í† í°í™” ë° ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        model_output = model(**inputs)
    
    # ë§ˆì§€ë§‰ íˆë“  ìŠ¤í…Œì´íŠ¸ ì‚¬ìš©
    last_hidden_state = model_output.last_hidden_state
    
    # ë§ˆì§€ë§‰ í† í°ì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    last_token_embedding = last_hidden_state[0, -1, :]
    
    # ì„ë² ë”©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    response = f"{last_token_embedding[:5].tolist()}"
    print(response)
def test_kakao_llm():
    llm = LLMModels()
    response = llm.generate("ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ê²Œ ì¸ì‚¬í•´ì¤˜")
   
    print(response)
    
def test_openai_embedding():
    API_KEY = settings.OPENAI_API_KEY
    if not API_KEY:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    embeddings = OpenAIEmbeddings(api_key=settings.OPENAI_API_KEY,
                                  model="text-embedding-ada-002")
    text = "DeepSeek ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì•„ì¡Œë‹¤ê³  ìµœê·¼ ê¸°ì‚¬ì— ë§ì´ ë‚˜ì˜¤ë„¤. ì–¼ë§ˆë‚˜ ì¢‹ì•„ì¡Œë‹ˆ?"
    query_result = embeddings.embed_query(text)
    print(query_result[:5])

def test_upstage_embedding():
    embed_service = EmbeddingService()
    embed_service.change_model(EmbeddingModelType.UPSTAGE)
    
    text = "DeepSeek ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì•„ì¡Œë‹¤ê³  ìµœê·¼ ê¸°ì‚¬ì— ë§ì´ ë‚˜ì˜¤ë„¤. ì–¼ë§ˆë‚˜ ì¢‹ì•„ì¡Œë‹ˆ?"
    query_result = embed_service.create_single_embedding(text)
    print(query_result[:5])
    print(f'dimension : {len(query_result)}')

def test_bgem3_embedding():
    from sentence_transformers import SentenceTransformer

    # Download from the ğŸ¤— Hub
    model = SentenceTransformer("dragonkue/bge-m3-ko")
    # Run inference
    sentences = [
        'ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œ ëŠ¥ë ¥ì´ ì—†ëŠ” ì„ì‚°ë¶€ëŠ” ëª‡ ì¢…ì— í•´ë‹¹í•˜ë‹ˆ?',
        'ë‚´ë…„ë¶€í„° ì €ì†Œë“ì¸µ 1ì„¸ ë¯¸ë§Œ ì•„ë™ì˜ \nì˜ë£Œë¹„ ë¶€ë‹´ì´ ë” ë‚®ì•„ì§„ë‹¤!\nì˜ë£Œê¸‰ì—¬ì œë„ ê°œìš”\nâ–¡ (ëª©ì ) ìƒí™œìœ ì§€ ëŠ¥ë ¥ì´ ì—†ê±°ë‚˜ ìƒí™œì´ ì–´ë ¤ìš´ êµ­ë¯¼ë“¤ì—ê²Œ ë°œìƒí•˜ëŠ” ì§ˆë³‘, ë¶€ìƒ, ì¶œì‚° ë“±ì— ëŒ€í•´ êµ­ê°€ê°€ ì˜ë£Œì„œë¹„ìŠ¤ ì œê³µ\nâ–¡ (ì§€ì›ëŒ€ìƒ) êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì, íƒ€ ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì ë“±\n\n| êµ¬ë¶„ | êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì | êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ë²• ì´ì™¸ì˜ íƒ€ ë²•ì— ì˜í•œ ìˆ˜ê¸‰ê¶Œì |\n| --- | --- | --- |\n| 1ì¢… | â—‹ êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œëŠ¥ë ¥ì´ ì—†ëŠ” ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê°€êµ¬ - 18ì„¸ ë¯¸ë§Œ, 65ì„¸ ì´ìƒ - 4ê¸‰ ì´ë‚´ ì¥ì• ì¸ - ì„ì‚°ë¶€, ë³‘ì—­ì˜ë¬´ì´í–‰ì ë“± | â—‹ ì´ì¬ë¯¼(ì¬í•´êµ¬í˜¸ë²•) â—‹ ì˜ìƒì ë° ì˜ì‚¬ìì˜ ìœ ì¡±â—‹ êµ­ë‚´ ì…ì–‘ëœ 18ì„¸ ë¯¸ë§Œ ì•„ë™â—‹ êµ­ê°€ìœ ê³µì ë° ê·¸ ìœ ì¡±â€¤ê°€ì¡±â—‹ êµ­ê°€ë¬´í˜•ë¬¸í™”ì¬ ë³´ìœ ì ë° ê·¸ ê°€ì¡±â—‹ ìƒˆí„°ë¯¼(ë¶í•œì´íƒˆì£¼ë¯¼)ê³¼ ê·¸ ê°€ì¡±â—‹ 5â€¤18 ë¯¼ì£¼í™”ìš´ë™ ê´€ë ¨ì ë° ê·¸ ìœ ê°€ì¡±â—‹ ë…¸ìˆ™ì¸ â€» í–‰ë ¤í™˜ì (ì˜ë£Œê¸‰ì—¬ë²• ì‹œí–‰ë ¹) |\n| 2ì¢… | â—‹ êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥ ìˆ˜ê¸‰ê¶Œì ì¤‘ ê·¼ë¡œëŠ¥ë ¥ì´ ìˆëŠ” ê°€êµ¬ | - |\n',
        'ì´ì–´ ì´ë‚  ì˜¤í›„ 1ì‹œ30ë¶„ë¶€í„° ì—´ë¦´ ì˜ˆì •ì´ë˜ ìŠ¤ë…¸ë³´ë“œ ì—¬ì ìŠ¬ë¡œí”„ìŠ¤íƒ€ì¼ ì˜ˆì„  ê²½ê¸°ëŠ” ì—°ê¸°ë¥¼ ê±°ë“­í•˜ë‹¤ ì·¨ì†Œëë‹¤. ì¡°ì§ìœ„ëŠ” ì˜ˆì„  ì—†ì´ ë‹¤ìŒ ë‚  ê²°ì„ ì—ì„œ ì°¸ê°€ì 27ëª…ì´ í•œë²ˆì— ê²½ê¸°í•´ ìˆœìœ„ë¥¼ ê°€ë¦¬ê¸°ë¡œ í–ˆë‹¤.',
    ]
    embeddings = model.encode(sentences)
    print(embeddings.shape)
    # [3, 1024]

    # Get the similarity scores for the embeddings
    similarities = model.similarity(embeddings, embeddings)
    print(similarities.shape)
    # [3, 3]
    print(embeddings[0][:5])
    
    # text = "DeepSeek ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì•„ì¡Œë‹¤ê³  ìµœê·¼ ê¸°ì‚¬ì— ë§ì´ ë‚˜ì˜¤ë„¤. ì–¼ë§ˆë‚˜ ì¢‹ì•„ì¡Œë‹ˆ?"
    # query_result = embed_service.create_single_embedding(text)
    # print(query_result[:5])
    # print(f'dimension : {len(query_result)}')

def test_langchain_google_embedding():
    credentials = service_account.Credentials.from_service_account_file(
        os.getenv("GOOGLE_APPLICATION_CREDENTIALS_VERTEXAI")
    )
    
    embeddings = VertexAIEmbeddings(
                                    model="text-multilingual-embedding-002",
                                    credentials=credentials)
    
    text = "DeepSeek ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì•„ì¡Œë‹¤ê³  ìµœê·¼ ê¸°ì‚¬ì— ë§ì´ ë‚˜ì˜¤ë„¤. ì–¼ë§ˆë‚˜ ì¢‹ì•„ì¡Œë‹ˆ?"
    #query_result = embeddings.embed_query(text)
    query_result = embeddings.embed([text], embeddings_task_type="SEMANTIC_SIMILARITY")
    
    print(query_result[0][:5])

    print(query_result[:5])
def test_langsmith():
    # Gemini ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatGoogleGenerativeAI(
        model="models/gemini-2.0-flash-exp",
        convert_system_message_to_human=True,
        temperature=0.7,
        google_api_key=settings.GEMINI_API_KEY,
    )
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ë©°, ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."),
        ("user", "{input}")
    ])
    
    # ì¶œë ¥ íŒŒì„œ ì •ì˜
    output_parser = StrOutputParser()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    questions = [
        "í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ ì´ë¦„ë§Œ ì•Œë ¤ì¤˜",
    ]
    
    for question in questions:
        print(f"\nì§ˆë¬¸: {question}")
        try:
            # 1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ ë©”ì‹œì§€ ìƒì„±
            formatted_messages = prompt_template.format_messages(input=question)
            
            # 2. LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            llm_response = llm.invoke(formatted_messages)
            
            # 3. ì¶œë ¥ íŒŒì„œë¡œ ì‘ë‹µ íŒŒì‹±
            #final_response = output_parser.invoke(llm_response.content)
            
            print(f"ì‘ë‹µ: {llm_response.content}\n")
            print("-" * 50)
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise


def test_get_gemini_models():
    # í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
    from google.ai import generativelanguage_v1beta
    import google.generativeai as genai
    genai.configure(api_key=settings.GEMINI_API_KEY) # YOUR_API_KEYë¥¼ ì‹¤ì œ API í‚¤ë¡œ ë³€ê²½

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    try:
        for model in genai.list_models():
            print(f"ëª¨ë¸ ì´ë¦„: {model.name}")
            print(f"í‘œì‹œ ì´ë¦„: {model.display_name}")
            print("---")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
def test_gemini_generate():
    llm = LLMModels()
    #print(f'settings.GEMINI_API_KEY: {settings.GEMINI_API_KEY}')

    prompt = """ë„ˆëŠ” ê¸ˆìœµ ë° ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ ë¶„ë¥˜ ì „ë¬¸ê°€ì´ì, LLM ê¸°ë°˜ ì§ˆë¬¸ë¶„ë¥˜ê¸° ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.
ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ì•„ë˜ì˜ ê¸°ì¤€ì— ë”°ë¼ ë¶„ì„í•˜ê³ , ê° í•­ëª©ì— ëŒ€í•´ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë„ì¶œí•´ì¤˜.

1. ì§ˆë¬¸ ì£¼ì œ:
   - [ì¢…ëª© ê¸°ë³¸ ì •ë³´]: ê¸°ì—…ì˜ ì¬ë¬´, ë°°ë‹¹, ì£¼ê°€, ì‹œì„¸ ë“± ê¸°ì´ˆ ì •ë³´ì™€ ê´€ë ¨ëœ ì§ˆë¬¸.
   - [ì „ë§ ê´€ë ¨]: ë¯¸ë˜ ì „ë§, íˆ¬ì ì˜ê²¬, ì‹œì¥ ë¶„ì„, ë¯¸ë˜ ì˜ˆì¸¡ ë“± ë¯¸ë˜ì˜ íë¦„ì´ë‚˜ ì „ëµì— ê´€í•œ ì§ˆë¬¸.
   - [ê¸°íƒ€]: ìœ„ ë‘ í•­ëª©ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê¸ˆìœµ/ì£¼ì‹ ê´€ë ¨ ê¸°íƒ€ ì§ˆë¬¸.

2. ë‹µë³€ ìš”êµ¬ ìˆ˜ì¤€:
   - [ê°„ë‹¨í•œ ë‹µë³€]: ë‹¨ìˆœ ì •ë³´, ìˆ«ì í˜¹ì€ ì§§ì€ ë‹¨ë‹µí˜• ë‹µë³€ì´ ì ì ˆí•œ ê²½ìš°.
   - [ê¸´ ì„¤ëª… ìš”êµ¬]: ë°°ê²½ ì •ë³´, ê·¼ê±° ë° ìƒì„¸ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš°.
   - [ì¢…í•©ì  íŒë‹¨]: ë‹¤ì–‘í•œ ë³€ìˆ˜ì™€ ë³µí•©ì  ìš”ì†Œë¥¼ ê³ ë ¤í•´ íŒë‹¨í•´ì•¼ í•˜ëŠ” ê²½ìš°.

3. ì¶”ê°€ ì˜µì…˜ (ì„ íƒ ì‚¬í•­):
   - ë§Œì•½ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ íŠ¹ì • DB ì¡°íšŒë‚˜ ì„ë² ë”© ê²€ìƒ‰ ì˜µì…˜ì´ ë‹¬ë¼ì ¸ì•¼ í•œë‹¤ë©´, ê·¸ì— ë§ëŠ” ì œì•ˆë„ í•¨ê»˜ ì œê³µí•´ì¤˜.
   - ì˜ˆë¥¼ ë“¤ì–´, "ì¢…ëª© ê¸°ë³¸ ì •ë³´"ì— í•´ë‹¹í•˜ë©´ ì¬ë¬´ ë°ì´í„°ë² ì´ìŠ¤, "ì „ë§ ê´€ë ¨"ì´ë¼ë©´ ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì°¸ê³ í•˜ëŠ” ì˜µì…˜ ë“±ì„ ì œì•ˆí•  ìˆ˜ ìˆìŒ.

ì‚¬ìš©ì ì§ˆë¬¸ ì˜ˆì‹œ: "Aê¸°ì—…ì˜ ë°°ë‹¹ë¥ ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
- ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ:
   - ì§ˆë¬¸ ì£¼ì œ: [ì¢…ëª© ê¸°ë³¸ ì •ë³´]
   - ë‹µë³€ ìš”êµ¬ ìˆ˜ì¤€: [ê°„ë‹¨í•œ ë‹µë³€]
   - ì¶”ê°€ ì˜µì…˜: ì¬ë¬´ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ

ì‚¬ìš©ì ì§ˆë¬¸ ì˜ˆì‹œ: "Bê¸°ì—…ì˜ í–¥í›„ ì„±ì¥ ê°€ëŠ¥ì„±ê³¼ ì‹œì¥ ì „ë§ì€ ì–´ë–»ê²Œ ë³´ì‹œë‚˜ìš”?"
- ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ:
   - ì§ˆë¬¸ ì£¼ì œ: [ì „ë§ ê´€ë ¨]
   - ë‹µë³€ ìš”êµ¬ ìˆ˜ì¤€: [ê¸´ ì„¤ëª… ìš”êµ¬] ë˜ëŠ” [ì¢…í•©ì  íŒë‹¨]
   - ì¶”ê°€ ì˜µì…˜: ì‹œì¥ ë¶„ì„ ë¦¬í¬íŠ¸ ë° ì „ë¬¸ê°€ ì˜ê²¬ ì„ë² ë”© ê²€ìƒ‰

ì´ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë„ì¶œí•´ì¤˜."""
    #response = llm.generate("ì‚¼ì„±ì „ì ì‚´ê¹Œ?ì§€ê¸ˆì´ë‹ˆ?", prompt)
    response = llm.generate("í•˜ì´ë‹‰ìŠ¤ 24ë…„ ë§¤ì¶œì€?", prompt)
    
    print(response.content)

    
async def test_func_aync():
    print(f"ENV : {settings.ENV}")
    from common.services.vector_store_manager import VectorStoreManager
    from common.services.embedding_models import EmbeddingModelType
    from stockeasy.services.telegram.rag import TelegramRAGService

    rag_service = TelegramRAGService()
    messages = await rag_service.search_messages("ë¡œë´‡ì£¼ë“¤ ê¸‰ë½ì´ ì‹¬í•œë°, ì™œ ì´ë˜?", 5)
    summary = await rag_service.summarize(messages)
    #summary= await rag_service.test_func()
    
    print(summary)
def test_question_classifier():
    from stockeasy.services.telegram.question_classifier import QuestionClassifierService
    question_classifier = QuestionClassifierService()
    # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°˜ë³µì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì
    # í„°ë¯¸ë„ë¡œ ì…ë ¥ë°›ê³ , q ì…ë ¥ì‹œ ë°˜ë³µ ì¢…ë£Œ
    while True:
        question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if question == "q" or question == "ã…‚":
            break
        result = question_classifier.classify_question(question)
        #result = question_classifier.classify_question_with_deberta(question)
        print(result)
        print(f"ì§ˆë¬¸ì£¼ì œ: {result['ì§ˆë¬¸ì£¼ì œ']}")
        print(f"ë‹µë³€ìˆ˜ì¤€: {result['ë‹µë³€ìˆ˜ì¤€']}")
        print(f"ì¶”ê°€ì˜µì…˜: {result['ì¶”ê°€ì˜µì…˜']}")
        print(f"ì¢…ëª©ì½”ë“œ: {result['ì¢…ëª©ì½”ë“œ']}")
        print(f"ì¢…ëª©ëª…: {result['ì¢…ëª©ëª…']}")
async def test_search_vectordb():
    from common.services.vector_store_manager import VectorStoreManager
    from common.services.embedding_models import EmbeddingModelType
    from common.services.retrievers.models import RetrievalResult
    from common.services.retrievers.semantic import SemanticRetriever, SemanticRetrieverConfig

    vs_manager = VectorStoreManager(embedding_model_type=EmbeddingModelType.GOOGLE_MULTI_LANG,
                                    project_name="stockeasy",
                                    namespace=settings.PINECONE_NAMESPACE_STOCKEASY_TELEGRAM)
    
    semantic_retriever = SemanticRetriever(config=SemanticRetrieverConfig(
                                                        min_score=0.6, # ìµœì†Œ ìœ ì‚¬ë„ 0.6 ê³ ì •
                                                        ), vs_manager=vs_manager)

    # ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ì„¤ì •
    start_date = datetime(2025, 1, 1)  # 2024ë…„ 1ì›” 1ì¼
    end_date = datetime(2025, 3, 3)   # 2024ë…„ 3ì›” 31ì¼

    # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    # Unix timestampë¡œ ë³€í™˜ (ì´ˆ ë‹¨ìœ„)
    start_timestamp = int(start_date.timestamp())
    end_timestamp = int(end_date.timestamp())

    # Pinecone í•„í„° ì¿¼ë¦¬
    filters = {
        "message_created_at": {
            "$gte": start_timestamp,
            "$lte": end_timestamp
        }
    }

    normalized_query = "í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤ ì‹¤ì ì´ ì™œ ì¢‹ì•˜ì§€?"
    # # document_idì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    # filters = {
    #     "created_at": {
    #         "$gte": start_date_str,
    #         "$lte": end_date_str
    #     },
    #     "document_id": {
    #         "$in": ["doc1", "doc2"]  # íŠ¹ì • ë¬¸ì„œ ID ëª©ë¡
    #     }
    # }
    all_chunks:RetrievalResult = await semantic_retriever.retrieve(
        query=normalized_query, 
        top_k=5,
        #filters=filters
    )
    # score ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_documents = sorted(all_chunks.documents, key=lambda x: x.score, reverse=True)

    print(f"\nê²€ìƒ‰ì–´: {normalized_query}")
    print("\n" + "="*100)
    
    for idx, doc in enumerate(sorted_documents, 1):
        # ISO í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±í•˜ê³  í•œêµ­ ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        message_created_at_data = doc.metadata.get('message_created_at', '')
        created_at = None
        
        # message_created_atì„ datetime ê°ì²´ë¡œ ë³€í™˜ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        if isinstance(message_created_at_data, str):
            # ISO í˜•ì‹ ë¬¸ìì—´ì¸ ê²½ìš°
            try:
                created_at = datetime.fromisoformat(message_created_at_data)
            except (ValueError, TypeError):
                # ISO í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                try:
                    # ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ ë¬¸ìì—´ì¸ì§€ í™•ì¸
                    created_at = datetime.fromtimestamp(float(message_created_at_data))
                except (ValueError, TypeError):
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                    created_at = datetime.now()
        elif isinstance(message_created_at_data, (int, float)):
            # ìœ ë‹‰ìŠ¤ íƒ€ì„ìŠ¤íƒ¬í”„ì¸ ê²½ìš°
            try:
                created_at = datetime.fromtimestamp(float(message_created_at_data))
            except (ValueError, TypeError):
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                created_at = datetime.now()
        else:
            # ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì¸ ê²½ìš° í˜„ì¬ ì‹œê°„ ì‚¬ìš©
            created_at = datetime.now()
        
        created_at_str = created_at.strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"\n[ê²€ìƒ‰ê²°ê³¼ {idx}, ì ìˆ˜:{doc.score}]")
        print(f"ì±„ë„ëª…: {doc.metadata.get('channel_title', 'ì±„ë„ëª… ì—†ìŒ')}")
        print(f"ì‘ì„±ì¼ì‹œ: {created_at_str}")
        print(f"ë‚´ìš©: {doc.page_content}")
        print("-"*100)
    
    print(f"\nì´ {len(all_chunks.documents)}ê°œì˜ ê²°ê³¼ê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    
    
    
    
if __name__ == "__main__":
    #print(os.getcwd())
    #test_google_storage_service()
    #asyncio.run(test_google_storage_service())
    #test_gemini_generate()
    #test_question_classifier()
    #test_upstage_embedding()
    test_get_gemini_models()
    #test_bgem3_embedding()
    #test_vertex_embedding()
    #test_kakao_embedding() 
    #test_kakao_llm()
    
    #test_kakao_gen()
    #test_openai()
    #test_func()
    #test_langchain_google_embedding()
    #asyncio.run(test_langsmith2())
    #asyncio.run(test_func_aync())
    #asyncio.run(test_search_vectordb())

