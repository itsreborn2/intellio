
import requests
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin
import re

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§ì„ ìœ„í•´ ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install selenium webdriver-manager")

class NotionCrawler:
    def __init__(self):
        """ë…¸ì…˜ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        # ì¼ë°˜ì ì¸ ë¸Œë¼ìš°ì € í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def extract_detailed_content(self, soup):
        """
        ë…¸ì…˜ í˜ì´ì§€ì—ì„œ ìƒì„¸í•œ ê³„ì¸µ êµ¬ì¡°ì™€ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œ
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            list: êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ë¦¬ìŠ¤íŠ¸
        """
        structured_content = []
        
        # ë…¸ì…˜ì˜ ê°œë³„ ë¸”ë¡ë“¤ì„ ì°¾ê¸°
        # ë…¸ì…˜ì€ ê° ì½˜í…ì¸  ë¸”ë¡ì„ data-block-id ì†ì„±ì„ ê°€ì§„ divë¡œ êµ¬ì„±
        content_blocks = soup.find_all('div', {'data-block-id': True})
        
        print(f"ğŸ” ë°œê²¬ëœ ê°œë³„ ì½˜í…ì¸  ë¸”ë¡: {len(content_blocks)}ê°œ")
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§‘í•©
        seen_texts = set()
        
        for i, block in enumerate(content_blocks):
            try:
                block_info = self.analyze_notion_block_improved(block, i+1)
                if block_info and block_info['text'].strip():
                    # ì¤‘ë³µ í…ìŠ¤íŠ¸ ì œê±°
                    text_key = block_info['text'].strip()[:100]  # ì²˜ìŒ 100ìë¡œ ì¤‘ë³µ ì²´í¬
                    if text_key not in seen_texts:
                        seen_texts.add(text_key)
                        structured_content.append(block_info)
            except Exception as e:
                print(f"âš ï¸ ë¸”ë¡ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ë§Œì•½ data-block-idë¡œ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if not structured_content:
            print("ğŸ”„ ëŒ€ì²´ ì„ íƒìë¡œ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„...")
            
            # ë…¸ì…˜ì˜ ë‹¤ë¥¸ ê°€ëŠ¥í•œ ì„ íƒìë“¤
            alternative_selectors = [
                '.notion-selectable',
                '[contenteditable]',
                '.notion-text-block',
                '.notion-header-block',
                '.notion-bulleted-list-item',
                '.notion-list-item'
            ]
            
            for selector in alternative_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"âœ… {selector}ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    for i, element in enumerate(elements):
                        try:
                            block_info = self.analyze_element_improved(element, i+1, selector)
                            if block_info and block_info['text'].strip():
                                text_key = block_info['text'].strip()[:100]
                                if text_key not in seen_texts:
                                    seen_texts.add(text_key)
                                    structured_content.append(block_info)
                        except Exception as e:
                            print(f"âš ï¸ ìš”ì†Œ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            continue
                    break
        
        return structured_content
    
    def analyze_notion_block_improved(self, block, index):
        """
        ê°œë³„ ë…¸ì…˜ ë¸”ë¡ì„ ê°œì„ ëœ ë°©ì‹ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ íƒ€ì…ê³¼ ì½˜í…ì¸  ì¶”ì¶œ
        
        Args:
            block: BeautifulSoup ìš”ì†Œ
            index: ë¸”ë¡ ì¸ë±ìŠ¤
            
        Returns:
            dict: ë¸”ë¡ ì •ë³´
        """
        # ë¸”ë¡ì˜ í´ë˜ìŠ¤ ì •ë³´ë¡œ íƒ€ì… íŒë‹¨
        class_list = block.get('class', [])
        
        # ë¸”ë¡ íƒ€ì… ê²°ì •
        block_type = 'unknown'
        if 'notion-header-block' in class_list:
            block_type = 'header'
        elif 'notion-text-block' in class_list:
            block_type = 'text'
        elif 'notion-bulleted-list-item' in class_list:
            block_type = 'bullet_list'
        elif 'notion-numbered-list-item' in class_list:
            block_type = 'numbered_list'
        elif 'notion-toggle-block' in class_list:
            block_type = 'toggle'
        elif 'notion-page-block' in class_list:
            block_type = 'page_title'
        elif 'notion-image-block' in class_list:
            block_type = 'image'
        
        # ê°œë³„ í…ìŠ¤íŠ¸ ìš”ì†Œë“¤ì„ ë¶„ë¦¬í•˜ì—¬ ì¶”ì¶œ
        text_parts = []
        
        # í—¤ë” ë¸”ë¡ì˜ ê²½ìš° ë” ì •í™•í•œ ì¶”ì¶œ ë°©ì‹ ì‚¬ìš©
        if block_type == 'header':
            # í—¤ë”ì˜ ê²½ìš° contenteditable ì†ì„±ì„ ê°€ì§„ ì§ì ‘ì ì¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            header_text_elements = block.find_all(attrs={'contenteditable': True})
            
            if header_text_elements:
                # ì²« ë²ˆì§¸ contenteditable ìš”ì†Œê°€ ì‹¤ì œ í—¤ë” í…ìŠ¤íŠ¸
                main_header = header_text_elements[0].get_text().strip()
                if main_header:
                    text_parts = [main_header]
            
            # contenteditableë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ëŒ€ì²´ ë°©ë²•
            if not text_parts:
                # data-content-editable-leaf ì†ì„± í™•ì¸
                leaf_elements = block.find_all(attrs={'data-content-editable-leaf': True})
                if leaf_elements:
                    for leaf in leaf_elements:
                        leaf_text = leaf.get_text().strip()
                        if leaf_text and len(leaf_text) > 3:
                            text_parts.append(leaf_text)
                            break  # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
            
            # ì—¬ì „íˆ ì°¾ì§€ ëª»í•œ ê²½ìš° ì§ì ‘ ìì‹ ìš”ì†Œ ì¤‘ ê°€ì¥ ì§§ì€ í…ìŠ¤íŠ¸ ì‚¬ìš© (í—¤ë” íŠ¹ì„±ìƒ ì§§ìŒ)
            if not text_parts:
                direct_children = list(block.children)
                shortest_text = ""
                min_length = float('inf')
                
                for child in direct_children:
                    if hasattr(child, 'get_text'):
                        child_text = child.get_text().strip()
                        if child_text and 10 < len(child_text) < min_length and len(child_text) < 200:
                            shortest_text = child_text
                            min_length = len(child_text)
                
                if shortest_text:
                    text_parts = [shortest_text]
        
        else:
            # í—¤ë”ê°€ ì•„ë‹Œ ë¸”ë¡ì˜ ê¸°ì¡´ ì²˜ë¦¬ ë°©ì‹
            # ì§ì ‘ì ì¸ í…ìŠ¤íŠ¸ ë…¸ë“œë“¤ ì°¾ê¸°
            direct_text_elements = block.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'span'], recursive=False)
            
            for element in direct_text_elements:
                # contenteditable ì†ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ìš°ì„ ì ìœ¼ë¡œ ì²˜ë¦¬
                if element.get('contenteditable') or element.get('data-content-editable-leaf'):
                    element_text = element.get_text().strip()
                    if element_text and len(element_text) > 3:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        text_parts.append(element_text)
            
            # ë§Œì•½ ì§ì ‘ì ì¸ í…ìŠ¤íŠ¸ê°€ ì—†ë‹¤ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not text_parts:
                full_text = block.get_text().strip()
                if full_text:
                    # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                    if len(full_text) > 200:
                        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
                        sentences = []
                        current_sentence = ""
                        for char in full_text:
                            current_sentence += char
                            if char in '.!?ã€‚' and len(current_sentence.strip()) > 10:
                                sentences.append(current_sentence.strip())
                                current_sentence = ""
                        if current_sentence.strip():
                            sentences.append(current_sentence.strip())
                        
                        # ë„ˆë¬´ ë§ì€ ë¬¸ì¥ì´ë©´ ì²˜ìŒ 3ê°œë§Œ
                        text_parts = sentences[:3] if len(sentences) > 3 else sentences
                    else:
                        text_parts = [full_text]
        
        # ìµœì¢… í…ìŠ¤íŠ¸ ê²°í•© (ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬)
        final_text = '\n'.join(text_parts) if text_parts else ""
        
        # í•˜ìœ„ ìš”ì†Œë“¤ ë¶„ì„
        sub_elements = []
        
        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œë“¤ ì°¾ê¸°
        list_items = block.find_all(['li', 'div'], class_=re.compile(r'notion.*list.*item'))
        for item in list_items:
            item_text = item.get_text().strip()
            if item_text and item_text not in final_text:  # ì¤‘ë³µ ì œê±°
                sub_elements.append({
                    'type': 'list_item',
                    'text': item_text
                })
        
        # ë§í¬ ì°¾ê¸°
        links = block.find_all('a', href=True)
        link_info = []
        for link in links:
            link_text = link.get_text().strip()
            if link_text:
                link_info.append({
                    'text': link_text,
                    'url': link.get('href')
                })
        
        return {
            'index': index,
            'type': block_type,
            'classes': class_list,
            'text': final_text,
            'text_parts': text_parts,  # ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ íŒŒíŠ¸ë“¤ë„ ì €ì¥
            'sub_elements': sub_elements,
            'links': link_info,
            'html_tag': block.name
        }
    
    def analyze_element_improved(self, element, index, selector):
        """
        ì¼ë°˜ ìš”ì†Œë¥¼ ê°œì„ ëœ ë°©ì‹ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì½˜í…ì¸  ì¶”ì¶œ
        
        Args:
            element: BeautifulSoup ìš”ì†Œ
            index: ìš”ì†Œ ì¸ë±ìŠ¤
            selector: ì‚¬ìš©ëœ ì„ íƒì
            
        Returns:
            dict: ìš”ì†Œ ì •ë³´
        """
        class_list = element.get('class', [])
        
        # ê°œë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_parts = []
        
        # contenteditable ìš”ì†Œ ìš°ì„  ì²˜ë¦¬
        editable_elements = element.find_all(attrs={'contenteditable': True}) or element.find_all(attrs={'data-content-editable-leaf': True})
        
        if editable_elements:
            for editable in editable_elements:
                text = editable.get_text().strip()
                if text and len(text) > 3:
                    text_parts.append(text)
        
        # editable ìš”ì†Œê°€ ì—†ìœ¼ë©´ ì§ì ‘ ìì‹ ìš”ì†Œë“¤ì—ì„œ ì¶”ì¶œ
        if not text_parts:
            for child in element.children:
                if hasattr(child, 'get_text'):
                    child_text = child.get_text().strip()
                    if child_text and len(child_text) > 3:
                        text_parts.append(child_text)
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸
        if not text_parts:
            full_text = element.get_text().strip()
            if full_text:
                text_parts = [full_text]
        
        # ìµœì¢… í…ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬)
        final_text = '\n'.join(text_parts) if text_parts else ""
        
        # ë§í¬ ì •ë³´ ì¶”ì¶œ
        links = element.find_all('a', href=True)
        link_info = []
        for link in links:
            link_text = link.get_text().strip()
            if link_text:
                link_info.append({
                    'text': link_text,
                    'url': link.get('href')
                })
        
        return {
            'index': index,
            'selector': selector,
            'classes': class_list,
            'text': final_text,
            'text_parts': text_parts,
            'links': link_info,
            'html_tag': element.name
        }
    
    def crawl_with_selenium(self, url):
        """
        Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë™ì  ì½˜í…ì¸ ê°€ í¬í•¨ëœ ë…¸ì…˜ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            url (str): í¬ë¡¤ë§í•  ë…¸ì…˜ í˜ì´ì§€ URL
            
        Returns:
            dict: ì¶”ì¶œëœ ì½˜í…ì¸  ì •ë³´
        """
        if not SELENIUM_AVAILABLE:
            print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. requests ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            return self.crawl_notion_page(url)
        
        driver = None
        try:
            print(f"ğŸ” Seleniumì„ ì‚¬ìš©í•œ ë…¸ì…˜ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {url}")
            
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¸°ê¸°
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            # ë“œë¼ì´ë²„ ì´ˆê¸°í™” (ìë™ìœ¼ë¡œ Chrome ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ)
            driver = webdriver.Chrome(service=webdriver.chrome.service.Service(ChromeDriverManager().install()), options=chrome_options)
            driver.set_page_load_timeout(30)
            
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            
            print("â³ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
            time.sleep(8)  # ë” ê¸´ ë¡œë”© ì‹œê°„
            
            # ìŠ¤í¬ë¡¤ì„ í†µí•´ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ
            print("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ...")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # ë…¸ì…˜ ì½˜í…ì¸ ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                wait = WebDriverWait(driver, 20)
                # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ì½˜í…ì¸  ë¡œë”© í™•ì¸
                selectors_to_try = [
                    "div[data-block-id]",
                    ".notion-page-content",
                    ".notion-selectable",
                    ".notion-header-block"
                ]
                
                content_found = False
                for selector in selectors_to_try:
                    try:
                        elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector)))
                        if elements:
                            print(f"âœ… ì½˜í…ì¸  ìš”ì†Œ ë°œê²¬: {selector} ({len(elements)}ê°œ)")
                            content_found = True
                            break
                    except TimeoutException:
                        continue
                
                if not content_found:
                    print("âš ï¸ íŠ¹ì • ì½˜í…ì¸  ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆì§€ë§Œ í˜ì´ì§€ íŒŒì‹±ì„ ê³„ì†í•©ë‹ˆë‹¤.")
                
            except TimeoutException:
                print("âš ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ, í˜„ì¬ ìƒíƒœë¡œ íŒŒì‹±ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            
            # í† ê¸€ ë²„íŠ¼ë“¤ì„ ì°¾ì•„ì„œ í´ë¦­í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  í¼ì¹˜ê¸°
            print("ğŸ”“ í† ê¸€ ì„¹ì…˜ë“¤ì„ í¼ì¹˜ëŠ” ì¤‘...")
            try:
                # í† ê¸€ ë²„íŠ¼ ì„ íƒìë“¤
                toggle_selectors = [
                    'div[aria-expanded="false"]',
                    'div[aria-label="ì—´ê¸°"]',
                    '.notion-list-item-box-left div[role="button"]'
                ]
                
                expanded_toggles = 0
                for selector in toggle_selectors:
                    try:
                        toggle_buttons = driver.find_elements(By.CSS_SELECTOR, selector)
                        print(f"ğŸ¯ ë°œê²¬ëœ í† ê¸€ ë²„íŠ¼ ({selector}): {len(toggle_buttons)}ê°œ")
                        
                        for i, button in enumerate(toggle_buttons):
                            try:
                                # ë²„íŠ¼ì´ í™”ë©´ì— ë³´ì´ëŠ”ì§€ í™•ì¸
                                if button.is_displayed() and button.is_enabled():
                                    # ìŠ¤í¬ë¡¤í•˜ì—¬ ë²„íŠ¼ì„ í™”ë©´ì— í‘œì‹œ
                                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", button)
                                    time.sleep(0.5)
                                    
                                    # í´ë¦­ ì‹œë„
                                    try:
                                        button.click()
                                        expanded_toggles += 1
                                        print(f"âœ… í† ê¸€ {i+1} í¼ì¹¨ ì™„ë£Œ")
                                        time.sleep(1)  # ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                                    except Exception as click_error:
                                        # JavaScriptë¡œ í´ë¦­ ì¬ì‹œë„
                                        try:
                                            driver.execute_script("arguments[0].click();", button)
                                            expanded_toggles += 1
                                            print(f"âœ… í† ê¸€ {i+1} í¼ì¹¨ ì™„ë£Œ (JS)")
                                            time.sleep(1)
                                        except Exception as js_error:
                                            print(f"âš ï¸ í† ê¸€ {i+1} í´ë¦­ ì‹¤íŒ¨: {js_error}")
                            except Exception as e:
                                print(f"âš ï¸ í† ê¸€ ë²„íŠ¼ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                continue
                        
                        if expanded_toggles > 0:
                            break  # í† ê¸€ì„ ì°¾ì•˜ìœ¼ë©´ ë‹¤ë¥¸ ì„ íƒìëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ
                            
                    except Exception as e:
                        print(f"âš ï¸ í† ê¸€ ì„ íƒì {selector} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                print(f"ğŸ“Š ì´ {expanded_toggles}ê°œì˜ í† ê¸€ ì„¹ì…˜ì„ í¼ì³¤ìŠµë‹ˆë‹¤.")
                
                if expanded_toggles > 0:
                    # í† ê¸€ì„ í¼ì¹œ í›„ ì¶”ê°€ ë¡œë”© ì‹œê°„
                    print("â³ í¼ì³ì§„ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°...")
                    time.sleep(5)
                    
                    # ë‹¤ì‹œ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  í™•ì¸
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                    driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(2)
                
            except Exception as e:
                print(f"âš ï¸ í† ê¸€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ì¶”ê°€ ë¡œë”© ì‹œê°„
            time.sleep(3)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
            print(f"ğŸ“„ í˜ì´ì§€ ì†ŒìŠ¤ í¬ê¸°: {len(page_source)} ë¬¸ì")
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
            crawl_result = {
                'url': url,
                'title': '',
                'structured_content': [],
                'images': [],
                'metadata': {},
                'method': 'selenium_detailed',
                'expanded_toggles': expanded_toggles
            }
            
            # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
            title_element = soup.find('title')
            if title_element:
                crawl_result['title'] = title_element.get_text().strip()
                print(f"ğŸ“Œ í˜ì´ì§€ ì œëª©: {crawl_result['title']}")
            
            # ìƒì„¸í•œ ì½˜í…ì¸  êµ¬ì¡° ì¶”ì¶œ
            crawl_result['structured_content'] = self.extract_detailed_content(soup)
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            images = soup.find_all('img', src=True)
            for img in images[:15]:  # ìƒìœ„ 15ê°œ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬
                src = img['src']
                alt = img.get('alt', '')
                full_url = urljoin(url, src)
                crawl_result['images'].append({
                    'src': full_url,
                    'alt': alt
                })
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                name = meta.get('name') or meta.get('property')
                content = meta.get('content')
                if name and content:
                    crawl_result['metadata'][name] = content
            
            print(f"ğŸ“Š ìƒì„¸ Selenium í¬ë¡¤ë§ ì™„ë£Œ:")
            print(f"   - êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ë¸”ë¡: {len(crawl_result['structured_content'])}ê°œ")
            print(f"   - ì´ë¯¸ì§€: {len(crawl_result['images'])}ê°œ")
            print(f"   - ë©”íƒ€ë°ì´í„°: {len(crawl_result['metadata'])}ê°œ")
            print(f"   - í¼ì³ì§„ í† ê¸€: {expanded_toggles}ê°œ")
            
            return crawl_result
            
        except WebDriverException as e:
            print(f"âŒ Selenium ë“œë¼ì´ë²„ ì˜¤ë¥˜: {e}")
            print("ğŸ”„ requests ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            return self.crawl_notion_page(url)
        except Exception as e:
            print(f"âŒ Selenium í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            print("ğŸ”„ requests ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            return self.crawl_notion_page(url)
        finally:
            if driver:
                driver.quit()
    
    def crawl_notion_page(self, url):
        """
        ë…¸ì…˜ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì½˜í…ì¸  ì¶”ì¶œ (requests ë°©ì‹)
        
        Args:
            url (str): í¬ë¡¤ë§í•  ë…¸ì…˜ í˜ì´ì§€ URL
            
        Returns:
            dict: ì¶”ì¶œëœ ì½˜í…ì¸  ì •ë³´
        """
        try:
            print(f"ğŸ” ë…¸ì…˜ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘ (requests): {url}")
            
            # í˜ì´ì§€ ìš”ì²­
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            print(f"âœ… HTTP ì‘ë‹µ ì½”ë“œ: {response.status_code}")
            print(f"ğŸ“„ ì½˜í…ì¸  íƒ€ì…: {response.headers.get('content-type', 'Unknown')}")
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
            crawl_result = {
                'url': url,
                'title': '',
                'content': '',
                'text_blocks': [],
                'links': [],
                'images': [],
                'metadata': {},
                'method': 'requests'
            }
            
            # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
            title_element = soup.find('title')
            if title_element:
                crawl_result['title'] = title_element.get_text().strip()
                print(f"ğŸ“Œ í˜ì´ì§€ ì œëª©: {crawl_result['title']}")
            
            # ë…¸ì…˜ íŠ¹ì • ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
            # ë…¸ì…˜ì€ ì£¼ë¡œ íŠ¹ì • í´ë˜ìŠ¤ë‚˜ IDë¥¼ ê°€ì§„ divì— ì½˜í…ì¸ ë¥¼ ë‹´ìŒ
            content_selectors = [
                'div[data-block-id]',  # ë…¸ì…˜ ë¸”ë¡
                '.notion-page-content',
                '.notion-page-block',
                '[role="main"]',
                'main',
                'article'
            ]
            
            content_elements = []
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_elements.extend(elements)
                    print(f"ğŸ¯ ë°œê²¬ëœ ì½˜í…ì¸  ë¸”ë¡ ({selector}): {len(elements)}ê°œ")
            
            # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
            if content_elements:
                for element in content_elements[:10]:  # ìƒìœ„ 10ê°œ ìš”ì†Œë§Œ ì²˜ë¦¬
                    text = element.get_text().strip()
                    if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì €ì¥
                        crawl_result['text_blocks'].append({
                            'tag': element.name,
                            'class': element.get('class', []),
                            'text': text[:500] + '...' if len(text) > 500 else text  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¶•ì•½
                        })
            else:
                # ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                body = soup.find('body')
                if body:
                    all_text = body.get_text()
                    # ê³µë°± ì •ë¦¬
                    clean_text = re.sub(r'\s+', ' ', all_text).strip()
                    crawl_result['content'] = clean_text[:1000] + '...' if len(clean_text) > 1000 else clean_text
                    print(f"ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(clean_text)} ë¬¸ì")
            
            # ë§í¬ ì¶”ì¶œ
            links = soup.find_all('a', href=True)
            for link in links[:20]:  # ìƒìœ„ 20ê°œ ë§í¬ë§Œ ì²˜ë¦¬
                href = link['href']
                text = link.get_text().strip()
                if text:
                    full_url = urljoin(url, href)
                    crawl_result['links'].append({
                        'text': text,
                        'url': full_url
                    })
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            images = soup.find_all('img', src=True)
            for img in images[:10]:  # ìƒìœ„ 10ê°œ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬
                src = img['src']
                alt = img.get('alt', '')
                full_url = urljoin(url, src)
                crawl_result['images'].append({
                    'src': full_url,
                    'alt': alt
                })
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                name = meta.get('name') or meta.get('property')
                content = meta.get('content')
                if name and content:
                    crawl_result['metadata'][name] = content
            
            print(f"ğŸ“Š requests í¬ë¡¤ë§ ì™„ë£Œ:")
            print(f"   - í…ìŠ¤íŠ¸ ë¸”ë¡: {len(crawl_result['text_blocks'])}ê°œ")
            print(f"   - ë§í¬: {len(crawl_result['links'])}ê°œ")
            print(f"   - ì´ë¯¸ì§€: {len(crawl_result['images'])}ê°œ")
            print(f"   - ë©”íƒ€ë°ì´í„°: {len(crawl_result['metadata'])}ê°œ")
            
            return crawl_result
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return None
    
    def save_text_only(self, result, filename="notion_content_text_only.txt"):
        """
        í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            result: í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            filename: ì €ì¥í•  íŒŒì¼ëª…
        """
        if not result or 'structured_content' not in result:
            print("âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œí•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                # í—¤ë” ì •ë³´ ì‘ì„±
                f.write("="*80 + "\n")
                f.write(f"ë…¸ì…˜ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼\n")
                f.write("="*80 + "\n\n")
                
                if result.get('title'):
                    f.write(f"ğŸ“Œ ì œëª©: {result['title']}\n")
                
                f.write(f"ğŸ”— URL: {result['url']}\n")
                f.write(f"ğŸ› ï¸ í¬ë¡¤ë§ ë°©ì‹: {result.get('method', 'unknown')}\n")
                
                if result.get('expanded_toggles'):
                    f.write(f"ğŸ”“ í¼ì³ì§„ í† ê¸€: {result['expanded_toggles']}ê°œ\n")
                
                f.write(f"ğŸ“„ ì´ ë¸”ë¡ ìˆ˜: {len(result['structured_content'])}ê°œ\n")
                f.write("\n" + "="*80 + "\n\n")
                
                # êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                current_header_level = 0
                
                for i, content in enumerate(result['structured_content'], 1):
                    block_type = content.get('type', 'unknown')
                    text = content.get('text', '').strip()
                    
                    if not text:
                        continue
                    
                    # ë¸”ë¡ íƒ€ì…ì— ë”°ë¥¸ í¬ë§·íŒ…
                    if block_type == 'page_title':
                        f.write(f"ğŸ·ï¸ {text}\n")
                        f.write("-" * len(text) + "\n\n")
                        
                    elif block_type == 'header':
                        f.write(f"\nğŸ“‹ {text}\n")
                        f.write("=" * min(len(text), 50) + "\n\n")
                        current_header_level = 1
                        
                    elif 'sub_header' in content.get('classes', []) or block_type == 'sub_header':
                        f.write(f"\nğŸ”¸ {text}\n")
                        f.write("-" * min(len(text), 30) + "\n\n")
                        current_header_level = 2
                        
                    elif 'bulleted_list' in content.get('classes', []) or block_type == 'bullet_list':
                        f.write(f"  â€¢ {text}\n")
                        
                    elif 'numbered_list' in content.get('classes', []) or block_type == 'numbered_list':
                        f.write(f"  {i}. {text}\n")
                        
                    elif 'quote' in content.get('classes', []) or block_type == 'quote':
                        f.write(f"\nğŸ’¬ \"{text}\"\n\n")
                        
                    elif 'table' in content.get('classes', []) or block_type == 'table':
                        f.write(f"\nğŸ“Š í‘œ ë°ì´í„°:\n{text}\n\n")
                        
                    elif block_type == 'text':
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë¬¸ë‹¨ìœ¼ë¡œ ì²˜ë¦¬
                        if len(text) > 20:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                            f.write(f"{text}\n\n")
                        
                    else:
                        # ê¸°íƒ€ ë¸”ë¡ë“¤
                        if len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                            f.write(f"{text}\n")
                    
                    # ë§í¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
                    if content.get('links'):
                        f.write("\nğŸ”— ê´€ë ¨ ë§í¬:\n")
                        for link in content['links'][:3]:  # ìƒìœ„ 3ê°œë§Œ
                            f.write(f"   - {link['text']}: {link['url']}\n")
                        f.write("\n")
                
                # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
                if result.get('images'):
                    f.write("\n" + "="*50 + "\n")
                    f.write("ğŸ–¼ï¸ í¬í•¨ëœ ì´ë¯¸ì§€ë“¤\n")
                    f.write("="*50 + "\n\n")
                    for i, img in enumerate(result['images'][:10], 1):
                        alt_text = img.get('alt', 'ì´ë¯¸ì§€')
                        f.write(f"{i}. {alt_text}\n")
                        f.write(f"   URL: {img['src']}\n\n")
                
                f.write("\n" + "="*80 + "\n")
                f.write("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ\n")
                f.write("="*80 + "\n")
            
            print(f"ğŸ“ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ {filename} íŒŒì¼ë¡œ ì €ì¥ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def extract_clean_text(self, result):
        """
        í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ë¦¬í„´ (ì„ë² ë”©ìš©)
        
        Args:
            result: í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            str: ê¹”ë”í•˜ê²Œ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not result or 'structured_content' not in result:
            return ""
        
        text_parts = []
        
        # ì œëª© ì¶”ê°€
        if result.get('title'):
            text_parts.append(f"{result['title']}\n")
        
        # êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for content in result['structured_content']:
            block_type = content.get('type', 'unknown')
            text = content.get('text', '').strip()
            
            if not text or len(text) < 5:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                continue
            
            # ë¸”ë¡ íƒ€ì…ì— ë”°ë¥¸ ê°„ë‹¨í•œ í¬ë§·íŒ…
            if block_type == 'page_title':
                text_parts.append(f"{text}\n")
                
            elif block_type == 'header':
                text_parts.append(f"\n{text}\n")
                
            elif 'sub_header' in content.get('classes', []) or block_type == 'sub_header':
                text_parts.append(f"\n{text}\n")
                
            elif 'bulleted_list' in content.get('classes', []) or block_type == 'bullet_list':
                text_parts.append(f"â€¢ {text}")
                
            elif 'numbered_list' in content.get('classes', []) or block_type == 'numbered_list':
                text_parts.append(f"{text}")
                
            elif 'quote' in content.get('classes', []) or block_type == 'quote':
                text_parts.append(f'"{text}"')
                
            elif 'table' in content.get('classes', []) or block_type == 'table':
                # í‘œ ë°ì´í„°ëŠ” í¬í•¨í•˜ë˜ í¬ë§·íŒ… ê°„ì†Œí™”
                text_parts.append(f"{text}")
                
            elif block_type == 'text':
                # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë¬¸ë‹¨ìœ¼ë¡œ ì²˜ë¦¬
                if len(text) > 15:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                    text_parts.append(f"{text}")
                    
            else:
                # ê¸°íƒ€ ë¸”ë¡ë“¤
                if len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                    text_parts.append(f"{text}")
        
        # ìµœì¢… í…ìŠ¤íŠ¸ ê²°í•©
        final_text = '\n'.join(text_parts)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        final_text = '\n'.join(line.strip() for line in final_text.split('\n') if line.strip())
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ)
        import re
        final_text = re.sub(r'\n{3,}', '\n\n', final_text)
        
        return final_text
    
    def print_crawl_result(self, result):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        if not result:
            print("âŒ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*80)
        print("ğŸ“‹ ë…¸ì…˜ í˜ì´ì§€ í¬ë¡¤ë§ ê²°ê³¼")
        print("="*80)
        
        print(f"\nğŸ”— URL: {result['url']}")
        print(f"ğŸ› ï¸ í¬ë¡¤ë§ ë°©ì‹: {result.get('method', 'unknown')}")
        
        if result.get('expanded_toggles'):
            print(f"ğŸ”“ í¼ì³ì§„ í† ê¸€: {result['expanded_toggles']}ê°œ")
        
        if result['title']:
            print(f"\nğŸ“Œ ì œëª©: {result['title']}")
        
        # êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ê°€ ìˆëŠ” ê²½ìš°
        if 'structured_content' in result and result['structured_content']:
            print(f"\nğŸ“„ êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ({len(result['structured_content'])}ê°œ ë¸”ë¡):")
            for i, content in enumerate(result['structured_content'][:30], 1):  # ìƒìœ„ 30ê°œë§Œ ì¶œë ¥
                print(f"\n   ğŸ“ ë¸”ë¡ {i} [{content.get('type', 'unknown')}]:")
                
                # text_partsê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
                if content.get('text_parts') and len(content['text_parts']) > 1:
                    print(f"      ğŸ“ í…ìŠ¤íŠ¸ íŒŒíŠ¸ë“¤ ({len(content['text_parts'])}ê°œ):")
                    for j, part in enumerate(content['text_parts'][:5], 1):  # ìƒìœ„ 5ê°œ íŒŒíŠ¸ë§Œ
                        preview = part[:100] + '...' if len(part) > 100 else part
                        print(f"        {j}. {preview}")
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶œë ¥
                    text_preview = content['text'][:200] + '...' if len(content['text']) > 200 else content['text']
                    # ì¤„ë°”ê¿ˆì„ ì‹¤ì œë¡œ í‘œì‹œ
                    formatted_text = text_preview.replace('\n', '\n           ')
                    print(f"      í…ìŠ¤íŠ¸: {formatted_text}")
                
                if content.get('sub_elements'):
                    print(f"      ğŸ“‹ í•˜ìœ„ ìš”ì†Œë“¤:")
                    for j, sub in enumerate(content['sub_elements'][:3], 1):
                        sub_preview = sub.get('text', '')[:80] + '...' if len(sub.get('text', '')) > 80 else sub.get('text', '')
                        print(f"        {j}. {sub_preview}")
                
                if content.get('links'):
                    print(f"      ğŸ”— ë§í¬ë“¤:")
                    for link in content['links'][:3]:
                        print(f"        ğŸ”— {link['text']} -> {link['url']}")
        
        # ê¸°ì¡´ text_blocksê°€ ìˆëŠ” ê²½ìš°
        elif 'text_blocks' in result and result['text_blocks']:
            print(f"\nğŸ“„ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ ({len(result['text_blocks'])}ê°œ):")
            for i, block in enumerate(result['text_blocks'][:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                print(f"\n   {i}. [{block['tag']}] {block['text']}")
        
        if result.get('content'):
            print(f"\nğŸ“ ì „ì²´ ì½˜í…ì¸  (ìš”ì•½):\n{result['content']}")
        
        if result.get('images'):
            print(f"\nğŸ–¼ï¸ ë°œê²¬ëœ ì´ë¯¸ì§€ë“¤ ({len(result['images'])}ê°œ):")
            for i, img in enumerate(result['images'][:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                print(f"   {i}. {img['alt']} -> {img['src']}")
        
        if result.get('metadata'):
            print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„° ({len(result['metadata'])}ê°œ):")
            for key, value in list(result['metadata'].items())[:10]:  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
                print(f"   {key}: {value}")
        
        print("\n" + "="*80)
