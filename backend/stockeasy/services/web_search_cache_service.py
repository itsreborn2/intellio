import csv
import os
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, insert, update, func, desc, text, bindparam
from sqlalchemy.dialects.postgresql import array
from datetime import datetime
from loguru import logger
from sqlalchemy.sql.expression import cast
from sqlalchemy import String

from common.services.embedding import EmbeddingService
from common.services.embedding_models import EmbeddingModelType
from stockeasy.models.web_search_cache import WebSearchQueryCache, WebSearchResultCache

class WebSearchCacheService:
    def __init__(self, db: AsyncSession):
        self.db = db
        self.similarity_threshold = 0.89  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì„ê³„ê°’
        self.csv_log_lock = asyncio.Lock() # CSV íŒŒì¼ ì ‘ê·¼ ë™ê¸°í™”ë¥¼ ìœ„í•œ Lock
        
    async def _log_similarity_to_csv(self, timestamp: datetime, original_query: str, found_query: str, similarity_score: float):
        """ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ë¥¼ CSV íŒŒì¼ì— ë¹„ë™ê¸°ì ìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤."""
        
        # íŒŒì¼ I/O ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì¤‘ì²© í•¨ìˆ˜
        def write_similarity_data():
            try:
                csv_dir = Path("stockeasy") / "local_cache" / "web_search"
                os.makedirs(csv_dir, exist_ok=True)
                
                date_str = timestamp.strftime('%Y%m%d')
                csv_path = csv_dir / f'similarity_checks_{date_str}.csv'
                
                formatted_timestamp = timestamp.strftime('%Y-%m-%d %H:%M:%S')
                data_row = [formatted_timestamp, original_query, found_query, f"{similarity_score:.4f}"]
                
                file_exists = csv_path.exists() and csv_path.stat().st_size > 0
                
                with open(csv_path, "a", newline="", encoding="utf-8-sig") as f: # utf-8-sigë¡œ ë³€ê²½í•˜ì—¬ Excelì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€
                    writer = csv.writer(f)
                    if not file_exists:
                        writer.writerow(["ì¼ì", "ì›ë³¸ì¿¼ë¦¬", "ì°¾ì€ì¿¼ë¦¬", "ìœ ì‚¬ë„ê°’"])  # í—¤ë”
                    writer.writerow(data_row)
                
                logger.debug(f"ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: {csv_path}")
                return str(csv_path)
            except Exception as e_io:
                logger.error(f"CSV ë¡œê·¸ íŒŒì¼ ì“°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e_io)}", exc_info=True)
                return None

        async with self.csv_log_lock:
            try:
                # íŒŒì¼ I/O ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                csv_file_path = await asyncio.to_thread(write_similarity_data)
                if csv_file_path:
                    logger.info(f"ğŸ’¾ ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ê°€ CSV íŒŒì¼ì— ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_file_path}")
                else:
                    logger.warning("ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼ CSV íŒŒì¼ ê¸°ë¡ ì‹¤íŒ¨.")
            except Exception as e_lock:
                # Lock ìì²´ ë˜ëŠ” to_thread í˜¸ì¶œ ê´€ë ¨ ì˜ˆì™¸ ì²˜ë¦¬
                logger.error(f"CSV ë¡œê·¸ ì €ì¥ ì¤‘ Lock ë˜ëŠ” ìŠ¤ë ˆë”© ì˜¤ë¥˜ ë°œìƒ: {str(e_lock)}", exc_info=True)
        
    async def check_cache(self, queries: List[str], stock_code: Optional[str] = None, stock_name: Optional[str] = None) -> Tuple[List[Dict[str, Any]], List[str]]:
        """
        ìºì‹œì—ì„œ ìœ ì‚¬í•œ ì¿¼ë¦¬ì™€ ê²°ê³¼ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ëª©ë¡
            stock_code: ì¢…ëª© ì½”ë“œ
            stock_name: ì¢…ëª© ì´ë¦„
            
        Returns:
            ìºì‹œ íˆíŠ¸ ê²°ê³¼ì™€ ìºì‹œ ë¯¸ìŠ¤ ì¿¼ë¦¬ì˜ íŠœí”Œ
        """
        if not queries:
            return [], []
        from sqlalchemy import func, cast, Float
        from sqlalchemy.dialects.postgresql import ARRAY

        # ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        embedding_service = EmbeddingService(model_type=EmbeddingModelType.OPENAI_3_LARGE)
        try:
            cache_hits = []
            cache_misses = []
            
            for query in queries:
                try:
                    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                    query_embedding = await embedding_service.create_single_embedding_async(query)
                    # ì„ë² ë”©ì„ pgvector í˜¸í™˜ ë¬¸ìì—´ë¡œ ë³€í™˜
                    embedding_str = "[" + ",".join(str(x) for x in query_embedding) + "]"
                    
                    # ë²¡í„° ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜í–‰ (raw SQL ì‚¬ìš©)
                    # 1 - (q.embedding <=> :embedding::vector) AS similarity,
                    # 1 - ì¬ã…”ê±°í•˜ê³  í•´ë³´ì.

                    stmt = text("""
                        SELECT 
                            q.id, 
                            q.query, 
                            q.stock_code,
                            q.stock_name,
                            1 - (q.embedding <=> (:embedding)::vector) AS similarity,
                            q.hit_count,
                            q.created_at
                        FROM 
                            stockeasy.web_search_query_cache q
                        WHERE 
                            q.stock_code = :stock_code
                        ORDER BY 
                            similarity DESC
                        LIMIT 5
                    """)
                    # ë˜ëŠ” ëª…ì‹œì  íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ì¶”ê°€
                    stmt = stmt.bindparams(
                        bindparam('embedding', type_=String),  # ë²¡í„° íƒ€ì…ì€ Noneìœ¼ë¡œ ì§€ì •
                        bindparam('stock_code', type_=String)  # ëª…ì‹œì  String íƒ€ì…
                    )
                    
                    # ì¿¼ë¦¬ ì‹¤í–‰
                    result = await self.db.execute(
                        stmt,
                        {
                            "embedding": embedding_str,  # ë¬¸ìì—´ ë³€í™˜ëœ ì„ë² ë”©
                            "stock_code": stock_code
                        }
                    )
                    similar_queries = result.fetchall()
                    
                    # ìœ ì‚¬ë„ í™•ì¸ ë° ìµœìƒìœ„ ê²°ê³¼ ì„ íƒ
                    cache_hit = False
                    for row in similar_queries:
                        similarity = row.similarity
                        logger.info(f"ìœ ì‚¬ë„ ë¹„êµ: ì…ë ¥ ì¿¼ë¦¬ '{query}', DB ì¿¼ë¦¬ '{row.query}', ìœ ì‚¬ë„: {similarity:.4f}")
                        # CSV ë¡œê·¸ ê¸°ë¡
                        await self._log_similarity_to_csv(datetime.utcnow(), query, row.query, similarity)
                        
                        if similarity >= self.similarity_threshold:  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” 
                            query_cache_id = row.id
                            
                            # ê²°ê³¼ ì¡°íšŒ
                            results_stmt = select(WebSearchResultCache).where(
                                WebSearchResultCache.query_cache_id == query_cache_id
                            )
                            results = await self.db.execute(results_stmt)
                            result_rows = results.scalars().all()
                            
                            if result_rows:
                                # ìºì‹œ íˆíŠ¸ ì—…ë°ì´íŠ¸
                                await self.update_hit_count(query_cache_id)
                                
                                # ê²°ê³¼ ë³€í™˜
                                for result_row in result_rows:
                                    cache_hits.append({
                                        "title": result_row.title,
                                        "content": result_row.content,
                                        "url": result_row.url,
                                        "search_query": query,
                                        "similarity_score": similarity
                                    })
                                
                                cache_hit = True
                                logger.info(f"ìºì‹œ íˆíŠ¸: ì¿¼ë¦¬ '{query}', ìœ ì‚¬ë„: {similarity:.4f}")
                                break
                    
                    if not cache_hit:
                        cache_misses.append(query)
                        logger.info(f"ìºì‹œ ë¯¸ìŠ¤: ì¿¼ë¦¬ '{query}'")
                        
                except Exception as e:
                    logger.error(f"ì¿¼ë¦¬ '{query}' ìºì‹œ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
                    cache_misses.append(query)
            
            return cache_hits, cache_misses
            
        finally:
            # ì„ë² ë”© ì„œë¹„ìŠ¤ ì •ë¦¬
            await embedding_service.aclose()
        
    async def save_to_cache(self, query: str, results: List[Dict[str, Any]], stock_code: Optional[str] = None, stock_name: Optional[str] = None) -> None:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            results: ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡
            stock_code: ì¢…ëª© ì½”ë“œ
            stock_name: ì¢…ëª© ì´ë¦„
        """
        if not query or not results:
            return
            
        try:
            # ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            embedding_service = EmbeddingService(model_type=EmbeddingModelType.OPENAI_3_LARGE)
            
            try:
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                query_embedding = await embedding_service.create_single_embedding_async(query)
                
                # ì¿¼ë¦¬ ìºì‹œ ì €ì¥
                query_cache = WebSearchQueryCache(
                    query=query,
                    stock_code=stock_code,
                    stock_name=stock_name,
                    embedding=query_embedding,
                    created_at=datetime.utcnow(),
                    hit_count=0
                )
                
                self.db.add(query_cache)
                await self.db.flush()
                
                # ê²°ê³¼ ìºì‹œ ì €ì¥
                for result in results:
                    result_cache = WebSearchResultCache(
                        query_cache_id=query_cache.id,
                        title=result.get("title"),
                        content=result.get("content"),
                        url=result.get("url"),
                        search_query=result.get("search_query", query),
                        search_date=datetime.utcnow()
                    )
                    self.db.add(result_cache)
                
                await self.db.commit()
                logger.info(f"ì¿¼ë¦¬ '{query}' ë° {len(results)} ê°œì˜ ê²°ê³¼ê°€ ìºì‹œì— ì €ì¥ë¨")
                
            finally:
                # ì„ë² ë”© ì„œë¹„ìŠ¤ ì •ë¦¬
                await embedding_service.aclose()
                
        except Exception as e:
            await self.db.rollback()
            logger.error(f"ì¿¼ë¦¬ '{query}' ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        
    async def update_hit_count(self, query_cache_id: int) -> None:
        """
        ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ë¥¼ ì¦ê°€ì‹œí‚¤ê³ , ë§ˆì§€ë§‰ íˆíŠ¸ ì‹œê°„ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            query_cache_id: ì¿¼ë¦¬ ìºì‹œ ID
        """
        try:
            stmt = (
                update(WebSearchQueryCache)
                .where(WebSearchQueryCache.id == query_cache_id)
                .values(
                    hit_count=WebSearchQueryCache.hit_count + 1,
                    last_hit_at=datetime.utcnow()
                )
            )
            await self.db.execute(stmt)
            await self.db.commit()
            
        except Exception as e:
            await self.db.rollback()
            logger.error(f"ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {query_cache_id}): {str(e)}", exc_info=True)
        
    async def cleanup_old_cache(self, max_age_days: int = 15, exclude_min_hits: int = 5) -> int:
        """
        ì˜¤ë˜ëœ ìºì‹œ í•­ëª©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            max_age_days: ìµœëŒ€ ë³´ê´€ ì¼ìˆ˜ (ê¸°ë³¸ê°’: 15ì¼)
            exclude_min_hits: ì´ ê°’ ì´ìƒì˜ íˆíŠ¸ ì¹´ìš´íŠ¸ë¥¼ ê°€ì§„ í•­ëª©ì€ ë³´ì¡´
            
        Returns:
            ì‚­ì œëœ í•­ëª© ìˆ˜
        """
        try:
            # max_age_daysì¼ ì „ ë‚ ì§œ ê³„ì‚°
            cutoff_date = datetime.utcnow() - datetime.timedelta(days=max_age_days)
            
            # ì‚­ì œ ëŒ€ìƒ ì¿¼ë¦¬ ID ì¡°íšŒ
            stmt = select(WebSearchQueryCache.id).where(
                WebSearchQueryCache.created_at < cutoff_date,
                WebSearchQueryCache.hit_count < exclude_min_hits
            )
            result = await self.db.execute(stmt)
            query_ids = [row[0] for row in result.fetchall()]
            
            if not query_ids:
                logger.info(f"ì‚­ì œí•  ì˜¤ë˜ëœ ìºì‹œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤ (ê¸°ì¤€: {max_age_days}ì¼ ì´ìƒ, íˆíŠ¸ {exclude_min_hits} ë¯¸ë§Œ)")
                return 0
            
            # ì‚­ì œ ì¿¼ë¦¬ ì‹¤í–‰ (ê²°ê³¼ëŠ” cascadeë¡œ ìë™ ì‚­ì œ)
            delete_stmt = (
                text("DELETE FROM stockeasy.web_search_query_cache WHERE id = ANY(:ids)")
                .bindparams(ids=query_ids)
            )
            result = await self.db.execute(delete_stmt)
            await self.db.commit()
            
            deleted_count = len(query_ids)
            logger.info(f"{deleted_count}ê°œì˜ ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì‚­ì œ ì™„ë£Œ (ê¸°ì¤€: {max_age_days}ì¼ ì´ìƒ, íˆíŠ¸ {exclude_min_hits} ë¯¸ë§Œ)")
            return deleted_count
            
        except Exception as e:
            await self.db.rollback()
            logger.error(f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return 0 