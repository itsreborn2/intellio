"""
ì§ˆë¬¸ ë¶„ì„ê¸° ì—ì´ì „íŠ¸ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì˜ë„, ì—”í‹°í‹°, í‚¤ì›Œë“œ ë“±ì˜
ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” QuestionAnalyzerAgent í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import os
import re
from datetime import datetime
from typing import Any, Dict, List, Literal, Optional
from typing import Optional as PydanticOptional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from loguru import logger
from pydantic import BaseModel, Field, validator
from sqlalchemy.ext.asyncio import AsyncSession

from common.core.config import settings
from common.core.redis import async_redis_client  # AsyncRedisClient í´ë˜ìŠ¤ ëŒ€ì‹  ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from common.models.token_usage import ProjectType
from common.services.agent_llm import get_agent_llm

# from langchain_tavily import TavilySearch
from common.services.tavily import TavilyService
from common.utils.util import extract_json_from_text, remove_json_block
from stockeasy.agents.base import BaseAgent
from stockeasy.models.agent_io import QuestionAnalysisResult
from stockeasy.prompts.question_analyzer_prompts import PROMPT_DYNAMIC_GENERAL_TOC, PROMPT_DYNAMIC_TOC, SYSTEM_PROMPT, format_question_analyzer_prompt
from stockeasy.services.financial.stock_info_service import StockInfoService


class Entities(BaseModel):
    """ì¶”ì¶œëœ ì—”í‹°í‹° ì •ë³´"""

    stock_name: PydanticOptional[str] = Field(None, description="ì¢…ëª©ëª… ë˜ëŠ” null")
    stock_code: PydanticOptional[str] = Field(None, description="ì¢…ëª©ì½”ë“œ ë˜ëŠ” null")
    sector: PydanticOptional[str] = Field(None, description="ì¢…ëª©ì´ ì†í•œ ì‚°ì—…/ì„¹í„° ë˜ëŠ” null")
    subgroup: PydanticOptional[List[str]] = Field(None, description="ì¢…ëª©ì´ ì†í•œ subgroup ë˜ëŠ” null")
    time_range: PydanticOptional[str] = Field(None, description="ì‹œê°„ë²”ìœ„ ë˜ëŠ” null")
    financial_metric: PydanticOptional[str] = Field(None, description="ì¬ë¬´ì§€í‘œ ë˜ëŠ” null")
    competitor: PydanticOptional[str] = Field(None, description="ê²½ìŸì‚¬ ë˜ëŠ” null")
    product: PydanticOptional[str] = Field(None, description="ì œí’ˆ/ì„œë¹„ìŠ¤ ë˜ëŠ” null")

    @validator("subgroup", pre=True)
    def validate_subgroup(cls, v):
        """subgroup í•„ë“œì˜ ì•ˆì „í•œ ì²˜ë¦¬ë¥¼ ìœ„í•œ validator"""
        if v is None or v == "null" or v == "":
            return None
        if isinstance(v, str):
            # ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹± ì‹œë„
            try:
                import json

                parsed = json.loads(v)
                if isinstance(parsed, list):
                    return parsed
            except (json.JSONDecodeError, ValueError):
                pass
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return None
        if isinstance(v, list):
            return v
        return None


class Classification(BaseModel):
    """ì§ˆë¬¸ ë¶„ë¥˜ ì •ë³´"""

    primary_intent: Literal["ì¢…ëª©ê¸°ë³¸ì •ë³´", "ì„±ê³¼ì „ë§", "ì¬ë¬´ë¶„ì„", "ì‚°ì—…ë™í–¥", "ê¸°íƒ€"] = Field(
        ...,
        description=(
            "ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤: "
            "'ì¢…ëª©ê¸°ë³¸ì •ë³´'(íšŒì‚¬ ê°œìš”, í˜„ì¬ ì£¼ê°€ ë“± ë‹¨ìˆœ ì •ë³´), "
            "'ì„±ê³¼ì „ë§'(ë¯¸ë˜ ì‹¤ì  ì˜ˆì¸¡, ëª©í‘œ ì£¼ê°€ ë“± ì˜ˆì¸¡/ì „ë§), "
            "'ì¬ë¬´ë¶„ì„'(ì¬ë¬´ì œí‘œ ìˆ˜ì¹˜, ë¹„ìœ¨ ë¶„ì„ ë“± ìƒì„¸ ë¶„ì„), "
            "'ì‚°ì—…ë™í–¥'(ê´€ë ¨ ì‚°ì—…/ì‹œì¥ ë¶„ì„, ê²½ìŸì‚¬ ë¹„êµ ë“±), "
            "'ê¸°íƒ€'(ìœ„ ë²”ì£¼ì— ì†í•˜ì§€ ì•Šê±°ë‚˜ ë¶„ë¥˜í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°). "
            "ì§ˆë¬¸ì˜ ì˜ë„ê°€ ëª¨í˜¸í•˜ë‹¤ë©´ ê°€ì¥ ì ì ˆí•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜ 'ê¸°íƒ€'ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”."
        ),
    )
    complexity: Literal["ë‹¨ìˆœ", "ì¤‘ê°„", "ë³µí•©", "ì „ë¬¸ê°€ê¸‰"] = Field(
        ...,
        description=(
            "ì§ˆë¬¸ì˜ ë³µì¡ë„ ìˆ˜ì¤€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤: "
            "'ë‹¨ìˆœ'(ë‹¨ì¼ ì •ë³´ ìš”ì²­, ì˜ˆ: 'í˜„ì¬ ì£¼ê°€ ì–¼ë§ˆì•¼?'), "
            "'ì¤‘ê°„'(ì—¬ëŸ¬ ì •ë³´ ê²°í•© ë˜ëŠ” ê°„ë‹¨í•œ ë¶„ì„ ìš”êµ¬, ì˜ˆ: 'ìµœê·¼ 1ë…„ ì£¼ê°€ ì¶”ì´ì™€ ì£¼ìš” ì´ìŠˆ ì•Œë ¤ì¤˜'), "
            "'ë³µí•©'(ë‹¤ê°ì  ë¶„ì„, ë¹„êµ, ê¹Šì€ ì´í•´ ìš”êµ¬, ì˜ˆ: 'ê²½ìŸì‚¬ ëŒ€ë¹„ ì¬ë¬´ ê±´ì „ì„±ê³¼ ì„±ì¥ ì „ë§ ë¶„ì„í•´ì¤˜'), "
            "'ì „ë¬¸ê°€ê¸‰'(ë§¤ìš° ì‹¬ì¸µì ì¸ ë¶„ì„, íŠ¹ì • ëª¨ë¸ë§/ê°€ì • ìš”êµ¬, ì˜ˆ: 'DCF ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ í–¥í›„ 5ë…„ ì˜ˆìƒ ì£¼ê°€ ì‚°ì¶œí•´ì¤˜'). "
            "ì§ˆë¬¸ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ë¶„ì„ ê¹Šì´ë¥¼ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì í•©í•œ ìˆ˜ì¤€ì„ ì„ íƒí•˜ì„¸ìš”."
        ),
    )
    expected_answer_type: Literal["ì‚¬ì‹¤í˜•", "ì¶”ë¡ í˜•", "ë¹„êµí˜•", "ì˜ˆì¸¡í˜•", "ì„¤ëª…í˜•", "ì¢…í•©í˜•"] = Field(
        ...,
        description=(
            "ì‚¬ìš©ìê°€ ê¸°ëŒ€í•˜ëŠ” ë‹µë³€ì˜ ìœ í˜•ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤: "
            "'ì‚¬ì‹¤í˜•'(ê°ê´€ì ì¸ ë°ì´í„°ë‚˜ ì •ë³´ ì „ë‹¬, ì˜ˆ: 'ì‘ë…„ ë§¤ì¶œì•¡ì€?'), "
            "'ì¶”ë¡ í˜•'(ì£¼ì–´ì§„ ì •ë³´ ê¸°ë°˜ì˜ ë…¼ë¦¬ì  ì¶”ë¡ /í•´ì„, ì˜ˆ: 'ìµœê·¼ ì‹¤ì  ë°œí‘œê°€ ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì€?'), "
            "'ë¹„êµí˜•'(ë‘˜ ì´ìƒì˜ ëŒ€ìƒì„ ë¹„êµ ë¶„ì„, ì˜ˆ: 'Aì‚¬ì™€ Bì‚¬ì˜ ìˆ˜ìµì„± ë¹„êµ'), "
            "'ì˜ˆì¸¡í˜•'(ë¯¸ë˜ ìƒíƒœë‚˜ ê²°ê³¼ ì˜ˆì¸¡, ì˜ˆ: 'ë‹¤ìŒ ë¶„ê¸° ì‹¤ì  ì „ë§ì€?'), "
            "'ì„¤ëª…í˜•'(ê°œë…, ì›ì¸, ê³¼ì • ë“±ì— ëŒ€í•œ ì„¤ëª…, ì˜ˆ: 'PERì´ ë¬´ì—‡ì¸ê°€ìš”?'), "
            "'ì¢…í•©í˜•'(ì—¬ëŸ¬ ìœ í˜•ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì œê³µ). "
            "ì§ˆë¬¸ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ê°€ì¥ ì í•©í•œ ë‹µë³€ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”."
        ),
    )


class DataRequirements(BaseModel):
    """ë°ì´í„° ìš”êµ¬ì‚¬í•­"""

    telegram_needed: bool = Field(..., description="í…”ë ˆê·¸ë¨ ë°ì´í„° í•„ìš” ì—¬ë¶€")
    reports_needed: bool = Field(..., description="ë¦¬í¬íŠ¸ ë°ì´í„° í•„ìš” ì—¬ë¶€")
    financial_statements_needed: bool = Field(..., description="ì¬ë¬´ì œí‘œ ë°ì´í„° í•„ìš” ì—¬ë¶€")
    industry_data_needed: bool = Field(..., description="ì‚°ì—… ë°ì´í„° í•„ìš” ì—¬ë¶€")
    confidential_data_needed: bool = Field(..., description="ë¹„ê³µê°œ ìë£Œ í•„ìš” ì—¬ë¶€")
    revenue_data_needed: bool = Field(False, description="ë§¤ì¶œ ë° ìˆ˜ì£¼ í˜„í™© ë°ì´í„° í•„ìš” ì—¬ë¶€")
    web_search_needed: bool = Field(False, description="ì›¹ ê²€ìƒ‰ ë°ì´í„° í•„ìš” ì—¬ë¶€,ê¸°ë³¸False")
    technical_analysis_needed: bool = Field(False, description="ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„° í•„ìš” ì—¬ë¶€")


class QuestionAnalysis(BaseModel):
    """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼"""

    entities: Entities = Field(..., description="ì¶”ì¶œëœ ì—”í‹°í‹° ì •ë³´")
    classification: Classification = Field(..., description="ì§ˆë¬¸ ë¶„ë¥˜ ì •ë³´")
    data_requirements: DataRequirements = Field(..., description="í•„ìš”í•œ ë°ì´í„° ì†ŒìŠ¤ ì •ë³´")
    keywords: List[str] = Field(..., description="ì¤‘ìš” í‚¤ì›Œë“œ ëª©ë¡")
    detail_level: Literal["ê°„ëµ", "ë³´í†µ", "ìƒì„¸"] = Field(..., description="ìš”êµ¬ë˜ëŠ” ìƒì„¸ë„")


class ConversationContextAnalysis(BaseModel):
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼"""

    requires_context: bool = Field(..., description="ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œì§€ ì—¬ë¶€")
    is_followup_question: bool = Field(..., description="ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€")
    referenced_context: PydanticOptional[str] = Field(None, description="ì°¸ì¡°í•˜ëŠ” ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)")
    relation_to_previous: Literal["ë…ë¦½ì ", "ì§ì ‘ì°¸ì¡°", "ê°„ì ‘ì°¸ì¡°", "í™•ì¥", "ìˆ˜ì •"] = Field(..., description="ì´ì „ ëŒ€í™”ì™€ì˜ ê´€ê³„")
    is_conversation_closing: bool = Field(False, description="ëŒ€í™” ë§ˆë¬´ë¦¬ë¥¼ ëœ»í•˜ëŠ” ì¸ì‚¬ë§ì¸ì§€ ì—¬ë¶€")
    closing_type: PydanticOptional[Literal["ê¸ì •ì ", "ì¤‘ë¦½ì ", "ë¶€ì •ì "]] = Field(None, description="ë§ˆë¬´ë¦¬ ì¸ì‚¬ ìœ í˜•")
    closing_response: PydanticOptional[str] = Field(None, description="ë§ˆë¬´ë¦¬ ì¸ì‚¬ì— ëŒ€í•œ ì‘ë‹µ ë©”ì‹œì§€")
    reasoning: str = Field(..., description="íŒë‹¨ì— ëŒ€í•œ ì´ìœ  ì„¤ëª…")
    is_different_stock: bool = Field(False, description="ì´ì „ ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ ì¢…ëª©ì— ê´€í•œ ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€")
    previous_stock_name: PydanticOptional[str] = Field(None, description="ì´ì „ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì¢…ëª©ëª…")
    previous_stock_code: PydanticOptional[str] = Field(None, description="ì´ì „ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì¢…ëª©ì½”ë“œ")
    stock_relation: PydanticOptional[Literal["ë™ì¼ì¢…ëª©", "ì¢…ëª©ë¹„êµ", "ë‹¤ë¥¸ì¢…ëª©", "ì•Œìˆ˜ì—†ìŒ"]] = Field(None, description="ì´ì „ ì¢…ëª©ê³¼ì˜ ê´€ê³„")


# ìƒˆë¡œìš´ ëª¨ë¸ í´ë˜ìŠ¤ ì¶”ê°€
class SubsectionModel(BaseModel):
    """
    í•˜ìœ„ ì„¹ì…˜ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ í¬ë§·
    """

    subsection_id: str = Field(description="í•˜ìœ„ ì„¹ì…˜ ID (ì˜ˆ: section_2_1)")
    title: str = Field(description="í•˜ìœ„ ì„¹ì…˜ ì œëª© (ì˜ˆ: 2.1 í•˜ìœ„ ì„¹ì…˜ ì œëª©)")
    description: Optional[str] = Field(default=None, description="í•˜ìœ„ ì„¹ì…˜ì—ì„œ ë‹¤ë£° ë‚´ìš©ì˜ ê°„ëµí•œ ì„¤ëª…")


class SectionModel(BaseModel):
    """
    ì„¹ì…˜ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ í¬ë§·
    """

    section_id: str = Field(description="ì„¹ì…˜ ID (ì˜ˆ: section_1)")
    title: str = Field(description="ì„¹ì…˜ ì œëª© (ì˜ˆ: 1. í•µì‹¬ ìš”ì•½)")
    description: Optional[str] = Field(default=None, description="ì„¹ì…˜ì—ì„œ ë‹¤ë£° ë‚´ìš©ì˜ ê°„ëµí•œ ì„¤ëª…")
    subsections: List[SubsectionModel] = Field(default_factory=list, description="í•˜ìœ„ ì„¹ì…˜ ëª©ë¡")


class DynamicTocOutput(BaseModel):
    """
    ë™ì  ëª©ì°¨ ìƒì„± ê²°ê³¼ë¥¼ ìœ„í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ í¬ë§·
    """

    title: str = Field(description="ë³´ê³ ì„œ ì œëª© (ì§ˆë¬¸ê³¼ ê¸°ì—…ëª…ì„ ë°˜ì˜)")
    sections: List[Dict[str, Any]] = Field(description="ë³´ê³ ì„œ ì„¹ì…˜ ì •ë³´")


class QuestionAnalyzerAgent(BaseAgent):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì—ì´ì „íŠ¸

    ì´ ì—ì´ì „íŠ¸ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    1. ì¢…ëª©ì½”ë“œ/ì¢…ëª©ëª… ë“± ì—”í‹°í‹° ì¶”ì¶œ
    2. ì§ˆë¬¸ì˜ ì˜ë„ ë¶„ë¥˜
    3. í•„ìš”í•œ ë°ì´í„° ìœ í˜• ì‹ë³„
    4. í‚¤ì›Œë“œ ì¶”ì¶œ
    """

    def __init__(self, name: Optional[str] = None, db: Optional[AsyncSession] = None):
        """
        ì§ˆë¬¸ ë¶„ì„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”

        Args:
            name: ì—ì´ì „íŠ¸ ì´ë¦„ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ í´ë˜ìŠ¤ëª… ì‚¬ìš©)
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ê°ì²´ (ì„ íƒì )
        """
        super().__init__(name, db)
        self.agent_llm = get_agent_llm("question_analyzer_agent")
        self.agent_llm_lite = get_agent_llm("gemini-2.5-flash-lite")  # get_agent_llm("gemini-2.0-flash-lite")
        logger.info(f"QuestionAnalyzerAgent initialized with provider: {self.agent_llm.get_provider()}, model: {self.agent_llm.get_model_name()}")
        self.prompt_template = SYSTEM_PROMPT

        # self.tavily_search = TavilySearch(api_key=settings.TAVILY_API_KEY)
        self.tavily_service = TavilyService()
        self.redis_client = async_redis_client  # ì§ì ‘ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹  ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

        # ê¸°ìˆ ì  ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜
        self.technical_analysis_keywords = {
            # ì°¨íŠ¸ íŒ¨í„´ í‚¤ì›Œë“œ
            "chart_patterns": [
                "ì°¨íŠ¸",
                "íŒ¨í„´",
                "ì§€ì§€ì„ ",
                "ì €í•­ì„ ",
                "ì¶”ì„¸ì„ ",
                "ì‚¼ê°í˜•íŒ¨í„´",
                "ë¨¸ë¦¬ì–´ê¹¨",
                "ìŒë°”ë‹¥",
                "ìŒì²œì •",
                "ì—­ì‚¼ê°í˜•",
                "ê¹ƒë°œíŒ¨í„´",
                "í˜ë„ŒíŠ¸",
                "ì›¨ì§€",
                "ì±„ë„",
                "ëŒíŒŒ",
                "ì´íƒˆ",
                "ë°˜ì „",
                "ì§€ì§€",
                "ì €í•­",
                "ì¶”ì„¸",
                "ìƒìŠ¹ì¶”ì„¸",
                "í•˜ë½ì¶”ì„¸",
                "íš¡ë³´",
            ],
            # ê¸°ìˆ ì  ì§€í‘œ í‚¤ì›Œë“œ
            "technical_indicators": [
                "RS",
                "ìƒëŒ€ê°•ë„",
                "MACD",
                "ë³¼ë¦°ì €ë°´ë“œ",
                "ì´ë™í‰ê· ì„ ",
                "ìŠ¤í† ìºìŠ¤í‹±",
                "ì´ë™í‰ê· ",
                "ë‹¨ìˆœì´ë™í‰ê· ",
                "ì§€ìˆ˜ì´ë™í‰ê· ",
                "SMA",
                "EMA",
                "ê°€ê²©ì´ë™í‰ê· ",
                "ê±°ë˜ëŸ‰",
                "ê±°ë˜ëŸ‰ì§€í‘œ",
                "OBV",
                "ì¶œë˜ëŸ‰ê· í˜•ì§€í‘œ",
                "ëª¨ë©˜í…€",
                "CCI",
                "ìœŒë¦¬ì—„ìŠ¤R",
                "í”¼ë³´ë‚˜ì¹˜",
                "ì¼ëª©ê· í˜•í‘œ",
                "ì—”ë²¨ë¡œí”„",
                "ADX",
                "ë°©í–¥ì„±ì§€ìˆ˜",
            ],
            # ë§¤ë§¤ ì‹ í˜¸ í‚¤ì›Œë“œ
            "trading_signals": [
                "ë§¤ìˆ˜ì‹ í˜¸",
                "ë§¤ë„ì‹ í˜¸",
                "ê³¨ë“ í¬ë¡œìŠ¤",
                "ë°ë“œí¬ë¡œìŠ¤",
                "ê³¼ë§¤ìˆ˜",
                "ê³¼ë§¤ë„",
                "ë§¤ìˆ˜íƒ€ì´ë°",
                "ë§¤ë„íƒ€ì´ë°",
                "ì§„ì…ì‹ í˜¸",
                "ì²­ì‚°ì‹ í˜¸",
                "ì‹ í˜¸",
                "í¬ë¡œìŠ¤",
                "ìƒí–¥ëŒíŒŒ",
                "í•˜í–¥ëŒíŒŒ",
                "ì‹ í˜¸ê°•ë„",
                "ë§¤ë§¤í¬ì§€ì…˜",
            ],
            # ê°€ê²© ì›€ì§ì„ í‚¤ì›Œë“œ
            "price_movements": [
                "ê°€ê²©ì›€ì§ì„",
                "ì£¼ê°€íë¦„",
                "ìƒìŠ¹ì„¸",
                "í•˜ë½ì„¸",
                "íš¡ë³´ì¥ì„¸",
                "ê¸‰ë“±",
                "ê¸‰ë½",
                "ì¡°ì •",
                "ë°˜ë“±",
                "ë°˜ë½",
                "ë³€ë™ì„±",
                "ê³ ì ",
                "ì €ì ",
                "ì‹ ê³ ê°€",
                "ì‹ ì €ê°€",
                "ê°­ìƒìŠ¹",
                "ê°­í•˜ë½",
                "ê°€ê²©ëŒ€",
                "êµ¬ê°„",
                "ë ˆë²¨",
            ],
            # ì‹œì¥ ë¶„ì„ í‚¤ì›Œë“œ
            "market_analysis": ["ê¸°ìˆ ì ë¶„ì„", "ì°¨íŠ¸ë¶„ì„", "í…Œí¬ë‹ˆì»¬ë¶„ì„", "ê¸°ìˆ ë¶„ì„", "ì°¨íŠ¸í•´ì„", "ê¸°ìˆ ì ê´€ì ", "ì°¨íŠ¸ìƒ", "ê¸°ìˆ ì ìš”ì¸", "ì°¨íŠ¸íŒ¨í„´ë¶„ì„", "ê¸°ìˆ ì ì‹ í˜¸"],
        }

    async def process(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì¤‘ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

        Args:
            state: í˜„ì¬ ìƒíƒœ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬

        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
            start_time = datetime.now()
            logger.info("QuestionAnalyzerAgent starting processing")

            # í˜„ì¬ ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ
            query = state.get("query", "")
            stock_code = state.get("stock_code", "")
            stock_name = state.get("stock_name", "")
            logger.info(f"query[{stock_name},{stock_code}] : {query}")

            if not query:
                logger.warning("Empty query provided to QuestionAnalyzerAgent")
                self._add_error(state, "ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                return state

            # ì¼ë°˜ ì§ˆë¬¸ ëª¨ë“œì¼ ë•Œ ì²˜ë¦¬
            if stock_code == "general":
                logger.info("ì¼ë°˜ ì§ˆë¬¸ ëª¨ë“œ: ì¢…ëª© ë…ë¦½ì  ë¶„ì„ ìˆ˜í–‰")
                return await self._handle_general_question(state, query, start_time)

            # state["agent_results"] = state.get("agent_results", {})
            # user_id ì¶”ì¶œ
            user_context = state.get("user_context", {})
            user_id = user_context.get("user_id", None)
            user_email = user_context.get("user_email", None)

            # ëŒ€í™” ê¸°ë¡ í™•ì¸
            conversation_history = state.get("conversation_history", [])
            logger.info(f"ëŒ€í™” ê¸°ë¡ íƒ€ì…: {type(conversation_history)}, ê¸¸ì´: {len(conversation_history) if isinstance(conversation_history, list) else 'ì•Œ ìˆ˜ ì—†ìŒ'}")

            has_valid_history = False  # ê°•ì œ ë¹„í™œì„±í™”

            if has_valid_history:
                logger.info(f"ëŒ€í™” ê¸°ë¡ ìˆìŒ: {len(conversation_history)}ê°œ ë©”ì‹œì§€")
                context_analysis = await self.analyze_conversation_context(query, conversation_history, stock_name, stock_code, user_id)
                context_analysis_result = context_analysis.model_dump()  # dict

                # ë¶„ì„ ê²°ê³¼ ìƒíƒœì— ì €ì¥
                state["context_analysis"] = context_analysis_result

                # ëŒ€í™” ë§ˆë¬´ë¦¬ ì¸ì‚¬ì¸ì§€ í™•ì¸
                if context_analysis.is_conversation_closing:
                    logger.info(f"ëŒ€í™” ë§ˆë¬´ë¦¬ ì¸ì‚¬ë¡œ ê°ì§€: ìœ í˜•={context_analysis.closing_type}")

                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    state["agent_results"]["question_analysis"] = context_analysis_result
                    state["summary"] = context_analysis.closing_response
                    state["answer"] = state["summary"]

                    # ë©”íŠ¸ë¦­ ê¸°ë¡ ë° ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()

                    state["metrics"] = state.get("metrics", {})
                    state["metrics"]["question_analyzer"] = {
                        "start_time": start_time,
                        "end_time": end_time,
                        "duration": duration,
                        "status": "completed",
                        "error": None,
                        "model_name": self.agent_llm.get_model_name(),
                    }

                    state["processing_status"] = state.get("processing_status", {})
                    state["processing_status"]["question_analyzer"] = "completed"

                    logger.info(f"ëŒ€í™” ë§ˆë¬´ë¦¬ ê°ì§€, QuestionAnalyzerAgent ë¹ ë¥¸ ì²˜ë¦¬ ì™„ë£Œ: {duration:.2f}ì´ˆ ì†Œìš”")
                    logger.info(f"ë§ˆë¬´ë¦¬ ìœ í˜•: {context_analysis.closing_type}, ì‘ë‹µ: {context_analysis.closing_response}")
                    return state

                if context_analysis.is_different_stock and context_analysis.stock_relation == "ë‹¤ë¥¸ì¢…ëª©":
                    logger.info("ì™„ì „íˆ ë‹¤ë¥¸ì¢…ëª© ì§ˆë¬¸")
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    state["agent_results"]["question_analysis"] = context_analysis_result
                    state["summary"] = "í˜„ì¬ ì¢…ëª©ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.\në‹¤ë¥¸ ì¢…ëª©ì— ê´€í•œ ì§ˆë¬¸ì€ ìƒˆ ì±„íŒ…ì—ì„œ í•´ì£¼ì„¸ìš”"
                    state["answer"] = state["summary"]
                    # ë©”íŠ¸ë¦­ ê¸°ë¡ ë° ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()

                    state["metrics"] = state.get("metrics", {})
                    state["metrics"]["question_analyzer"] = {
                        "start_time": start_time,
                        "end_time": end_time,
                        "duration": duration,
                        "status": "completed",
                        "error": None,
                        "model_name": self.agent_llm.get_model_name(),
                    }

                    state["processing_status"] = state.get("processing_status", {})
                    state["processing_status"]["question_analyzer"] = "completed"

                    logger.info(f"ëŒ€í™” ë§ˆë¬´ë¦¬ ê°ì§€, QuestionAnalyzerAgent ë¹ ë¥¸ ì²˜ë¦¬ ì™„ë£Œ: {duration:.2f}ì´ˆ ì†Œìš”")
                    logger.info(f"ë§ˆë¬´ë¦¬ ìœ í˜•: {context_analysis.closing_type}, ì‘ë‹µ: {context_analysis.closing_response}")
                    return state

                # í›„ì† ì§ˆë¬¸ì¸ ê²½ìš° ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ê³  ë¦¬í„´
                if context_analysis.requires_context:
                    logger.info("í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê°ì§€ë˜ì–´ ìƒì„¸ ë¶„ì„ ìƒëµí•˜ê³  ë¹ ë¥´ê²Œ ë¦¬í„´í•©ë‹ˆë‹¤.")

                    state["agent_results"]["question_analysis"] = context_analysis_result

                    # ë©”íŠ¸ë¦­ ê¸°ë¡ ë° ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()

                    state["metrics"] = state.get("metrics", {})
                    state["metrics"]["question_analyzer"] = {
                        "start_time": start_time,
                        "end_time": end_time,
                        "duration": duration,
                        "status": "completed",
                        "error": None,
                        "model_name": self.agent_llm.get_model_name(),
                    }

                    state["processing_status"] = state.get("processing_status", {})
                    state["processing_status"]["question_analyzer"] = "completed"

                    logger.info(f"QuestionAnalyzerAgent ë¹ ë¥¸ ì²˜ë¦¬ ì™„ë£Œ: {duration:.2f}ì´ˆ ì†Œìš”")
                    return state

                # í›„ì† ì²˜ë¦¬ì— í•„ìš”í•œ ì •ë³´ ê¸°ë¡
                if context_analysis.requires_context:
                    logger.info(f"ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° í•„ìš”: {context_analysis.relation_to_previous}")
                    if context_analysis.referenced_context:
                        logger.info(f"ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸: {context_analysis.referenced_context}")
            else:
                logger.info("ëŒ€í™” ê¸°ë¡ ì—†ìŒ, ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê±´ë„ˆëœ€")
                state["context_analysis"] = {
                    "requires_context": False,
                    "is_followup_question": False,
                    "relation_to_previous": "ë…ë¦½ì ",
                    "is_conversation_closing": False,
                    "closing_type": None,
                    "closing_response": None,
                    "is_different_stock": False,
                    "previous_stock_name": None,
                    "previous_stock_code": None,
                    "stock_relation": "ì•Œìˆ˜ì—†ìŒ",
                    "reasoning": "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.",
                }

            logger.info(f"QuestionAnalyzerAgent analyzing query: {query}")

            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™•ì¸
            # 1. ìƒíƒœì—ì„œ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™•ì¸
            custom_prompt_from_state = state.get("custom_prompt_template")
            # 2. ì†ì„±ì—ì„œ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™•ì¸
            custom_prompt_from_attr = getattr(self, "prompt_template_test", None)

            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ìš°ì„ ìˆœìœ„: ìƒíƒœ > ì†ì„± > ê¸°ë³¸ê°’
            system_prompt = None
            if custom_prompt_from_state:
                system_prompt = custom_prompt_from_state
                logger.info(f"QuestionAnalyzerAgent using custom prompt from state : {custom_prompt_from_state}")
            elif custom_prompt_from_attr:
                system_prompt = custom_prompt_from_attr
                logger.info("QuestionAnalyzerAgent using custom prompt from attribute")

            import asyncio

            # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë° 2. ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰/ëª©ì°¨ ìƒì„±ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            logger.info("ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ê³¼ ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰")

            # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ë¹„ë™ê¸° í•¨ìˆ˜
            async def analyze_question_intent():
                # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
                prompt = format_question_analyzer_prompt(query=query, stock_name=stock_name, stock_code=stock_code, system_prompt=system_prompt)

                try:
                    # LLM í˜¸ì¶œë¡œ ë¶„ì„ ìˆ˜í–‰
                    # agent_temp = get_agent_llm("gemini-2.0-flash")

                    # raw_response = await agent_temp.with_structured_output(QuestionAnalysis).ainvoke(prompt, user_id=user_id, project_type=ProjectType.STOCKEASY, db=self.db)
                    raw_response = await self.agent_llm_lite.with_structured_output(QuestionAnalysis).ainvoke(
                        prompt, user_id=user_id, project_type=ProjectType.STOCKEASY, db=self.db
                    )

                    response: QuestionAnalysis

                    if isinstance(raw_response, AIMessage):
                        logger.info("AIMessage í˜•íƒœë¡œ ì‘ë‹µ ë°›ìŒ, JSON íŒŒì‹± ì‹œë„")
                        content = raw_response.content

                        # ```json ``` ì œê±°
                        if isinstance(content, str):
                            # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ```json ... ``` ë˜ëŠ” ``` ... ``` íŒ¨í„´ì„ ì°¾ê³  ë‚´ë¶€ JSONë§Œ ì¶”ì¶œ
                            match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", content, re.DOTALL)
                            if match:
                                json_str = match.group(1)
                            else:
                                # ë‹¨ìˆœ ``` ì œê±° ì‹œë„ (íŒ¨í„´ì´ ì•ˆë§ì„ ê²½ìš° ëŒ€ë¹„)
                                json_str = content.strip()
                                if json_str.startswith("```json"):
                                    json_str = json_str[7:]
                                if json_str.startswith("```"):
                                    json_str = json_str[3:]
                                if json_str.endswith("```"):
                                    json_str = json_str[:-3]
                                json_str = json_str.strip()

                            try:
                                parsed_data = json.loads(json_str)
                                response = QuestionAnalysis(**parsed_data)
                                logger.info(f"AIMessage JSON íŒŒì‹± ì„±ê³µ: {response}")
                            except json.JSONDecodeError as json_err:
                                logger.error(f"AIMessage JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}. Fallback ë¡œì§ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                                raise Exception("AIMessage JSON parsing failed")  # Fallback íŠ¸ë¦¬ê±°
                        else:
                            logger.error("AIMessage contentê°€ ë¬¸ìì—´ì´ ì•„ë‹˜. Fallback ë¡œì§ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                            raise Exception("AIMessage content is not a string")  # Fallback íŠ¸ë¦¬ê±°
                    elif isinstance(raw_response, QuestionAnalysis):
                        response = raw_response
                    else:
                        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ íƒ€ì…: {type(raw_response)}. Fallback ë¡œì§ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                        raise Exception(f"Unexpected response type: {type(raw_response)}")

                    # responseê°€ QuestionAnalysis ê°ì²´ì¸ ê²½ìš° ì²˜ë¦¬
                    response.entities.stock_name = stock_name
                    response.entities.stock_code = stock_code

                    # ëª¨ë“  ë°ì´í„° ì „ë¶€ on
                    response.data_requirements.reports_needed = True
                    response.data_requirements.telegram_needed = True
                    response.data_requirements.financial_statements_needed = True
                    response.data_requirements.industry_data_needed = True
                    response.data_requirements.confidential_data_needed = True
                    response.data_requirements.revenue_data_needed = True

                    # ê¸°ìˆ ì  ë¶„ì„ì€ ë¬´ì¡°ê±´ í•„ìš”ë¡œ ì„¤ì • (í‚¤ì›Œë“œ ê°ì§€ì™€ ê´€ê³„ì—†ì´)
                    ta_needed = True  # self._detect_technical_analysis_need(query) ëŒ€ì‹  ë¬´ì¡°ê±´ True
                    response.data_requirements.technical_analysis_needed = ta_needed
                    logger.info(f"[ë°ì´í„°ìš”êµ¬ì‚¬í•­] technical_analysis_needed ì„¤ì •: {ta_needed} (ë¬´ì¡°ê±´ í™œì„±í™”)")

                    # ë¶„ì„ ê²°ê³¼ ë¡œê¹…
                    logger.info(f"Analysis result: {response}")

                    # ì„œë¸Œê·¸ë£¹ ê°€ì ¸ì˜¤ê¸°
                    stock_info_service = StockInfoService()
                    subgroup_list = await stock_info_service.get_sector_by_code(stock_code)
                    logger.info(f"subgroup_info: {subgroup_list}")

                    if subgroup_list and len(subgroup_list) > 0:
                        response.entities.subgroup = subgroup_list

                    # QuestionAnalysisResult ê°ì²´ ìƒì„± - ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì‚¬ìš©
                    question_analysis: QuestionAnalysisResult = {
                        "entities": response.entities.dict(),
                        "classification": response.classification.dict(),
                        "data_requirements": response.data_requirements.dict(),
                        "keywords": response.keywords,
                        "detail_level": response.detail_level,
                    }

                except Exception as e:
                    # êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹±ì— ì‹¤íŒ¨í•œ ê²½ìš° fallback ì²˜ë¦¬
                    logger.error(f"êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

                    # LLM í˜¸ì¶œ ë‹¤ì‹œ ì‹œë„ (ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ)
                    logger.info("ì¼ë°˜ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
                    ai_response: AIMessage = await self.agent_llm.ainvoke_with_fallback(prompt, user_id=user_id, project_type=ProjectType.STOCKEASY, db=self.db)

                    logger.info(f"ì¼ë°˜ ì‘ë‹µ ë°›ìŒ: {type(ai_response)}")

                    # AIMessageì—ì„œ JSON íŒŒì‹± ì‹œë„
                    try:
                        # JSON íŒ¨í„´ ì°¾ê¸° (ì¤‘ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ë¶€ë¶„) - re ëª¨ë“ˆì€ ì´ë¯¸ ìµœìƒë‹¨ì— importë¨
                        json_str = extract_json_from_text(ai_response.content)

                        if json_str:
                            # JSON ë¬¸ìì—´ íŒŒì‹±
                            parsed_data = json.loads(json_str)
                            logger.info(f"\nâœ… JSON íŒŒì‹± ì„±ê³µ: title={parsed_data.get('title')}, sections={len(parsed_data.get('sections', []))}ê°œ")

                            # ê¸°ë³¸ ë°ì´í„° êµ¬ì¡° ìƒì„±
                            question_analysis = {
                                "entities": {
                                    "stock_name": stock_name,
                                    "stock_code": stock_code,
                                    "sector": parsed_data.get("entities", {}).get("sector"),
                                    "subgroup": None,  # ì•„ë˜ì—ì„œ ì„¤ì •
                                    "time_range": parsed_data.get("entities", {}).get("time_range"),
                                    "financial_metric": parsed_data.get("entities", {}).get("financial_metric"),
                                    "competitor": parsed_data.get("entities", {}).get("competitor"),
                                    "product": parsed_data.get("entities", {}).get("product"),
                                },
                                "classification": {
                                    "primary_intent": parsed_data.get("classification", {}).get("primary_intent", "ì¢…ëª©ê¸°ë³¸ì •ë³´"),
                                    "complexity": parsed_data.get("classification", {}).get("complexity", "ì¤‘ê°„"),
                                    "expected_answer_type": parsed_data.get("classification", {}).get("expected_answer_type", "ì‚¬ì‹¤í˜•"),
                                },
                                "data_requirements": {
                                    "telegram_needed": True,
                                    "reports_needed": True,
                                    "financial_statements_needed": True,
                                    "industry_data_needed": True,
                                    "confidential_data_needed": True,
                                    "revenue_data_needed": True,
                                    "web_search_needed": parsed_data.get("data_requirements", {}).get("web_search_needed", False),
                                    "technical_analysis_needed": True,  # ë¬´ì¡°ê±´ í™œì„±í™”
                                },
                                "keywords": parsed_data.get("keywords", []),
                                "detail_level": parsed_data.get("detail_level", "ë³´í†µ"),
                            }

                            # ì„œë¸Œê·¸ë£¹ ê°€ì ¸ì˜¤ê¸°
                            stock_info_service = StockInfoService()
                            subgroup_list = await stock_info_service.get_sector_by_code(stock_code)
                            logger.info(f"subgroup_info: {subgroup_list}")

                            if subgroup_list and len(subgroup_list) > 0:
                                question_analysis["entities"]["subgroup"] = subgroup_list
                        else:
                            logger.warning("JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ì‚¬ìš©")
                            # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ìƒì„±
                            question_analysis = await create_default_question_analysis(stock_name, stock_code)

                    except Exception as json_error:
                        logger.error(f"JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(json_error)}")
                        # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ìƒì„±
                        question_analysis = await create_default_question_analysis(stock_name, stock_code)

                return question_analysis

            # ê¸°ë³¸ ì§ˆë¬¸ ë¶„ì„ êµ¬ì¡° ìƒì„± í•¨ìˆ˜
            async def create_default_question_analysis(stock_name, stock_code):
                # ì„œë¸Œê·¸ë£¹ ê°€ì ¸ì˜¤ê¸°
                try:
                    stock_info_service = StockInfoService()
                    subgroup_list = await stock_info_service.get_sector_by_code(stock_code)
                except Exception:
                    subgroup_list = []

                # ê¸°ìˆ ì  ë¶„ì„ì€ ë¬´ì¡°ê±´ í•„ìš”ë¡œ ì„¤ì • (ê¸°ë³¸ êµ¬ì¡°ì—ì„œë„)
                ta_needed_default = True  # self._detect_technical_analysis_need(query) ëŒ€ì‹  ë¬´ì¡°ê±´ True
                logger.info(f"[ê¸°ë³¸ë¶„ì„êµ¬ì¡°] ê¸°ìˆ ì ë¶„ì„ í•„ìš”ì„± ì„¤ì •: {ta_needed_default} (ë¬´ì¡°ê±´ í™œì„±í™”)")

                return {
                    "entities": {
                        "stock_name": stock_name,
                        "stock_code": stock_code,
                        "sector": None,
                        "subgroup": subgroup_list if subgroup_list and len(subgroup_list) > 0 else None,
                        "time_range": None,
                        "financial_metric": None,
                        "competitor": None,
                        "product": None,
                    },
                    "classification": {"primary_intent": "ì¢…ëª©ê¸°ë³¸ì •ë³´", "complexity": "ì¤‘ê°„", "expected_answer_type": "ì‚¬ì‹¤í˜•"},
                    "data_requirements": {
                        "telegram_needed": True,
                        "reports_needed": True,
                        "financial_statements_needed": True,
                        "industry_data_needed": True,
                        "confidential_data_needed": True,
                        "revenue_data_needed": True,
                        "web_search_needed": False,
                        "technical_analysis_needed": ta_needed_default,
                    },
                    "keywords": [stock_name, "ì •ë³´"],
                    "detail_level": "ë³´í†µ",
                }

            # 2. ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ ë° ëª©ì°¨ ìƒì„± ë¹„ë™ê¸° í•¨ìˆ˜
            async def search_issues_and_generate_toc():
                # logger.info(f"search_issues_and_generate_toc ì‹¤í–‰: {user_email} - {settings.ADMIN_IDS}, {user_email in settings.ADMIN_IDS}")
                is_admin_and_prod = settings.ENV == "production" and user_email in settings.ADMIN_IDS
                redis_client = self.redis_client
                cache_key_prefix = "recent_issues_summary"
                cache_key = f"{cache_key_prefix}:{stock_name}:{stock_code}"

                recent_issues_summary = None

                # 1. ê´€ë¦¬ìê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ìºì‹œì—ì„œ ì¡°íšŒ
                if not is_admin_and_prod:
                    cached_summary = await redis_client.get_key(cache_key)
                    if cached_summary:
                        logger.info(f"ì¢…ëª© [{stock_name}/{stock_code}]ì— ëŒ€í•œ ìºì‹œëœ ìµœê·¼ ì´ìŠˆ ìš”ì•½ ì‚¬ìš©: {cache_key}")
                        recent_issues_summary = cached_summary

                # 2. ìºì‹œì— ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ê´€ë¦¬ìì¸ ê²½ìš°, ìƒˆë¡œ ìƒì„±í•˜ê³  ìºì‹œì— ì €ì¥
                if recent_issues_summary is None:
                    if is_admin_and_prod:
                        logger.info(f"ê´€ë¦¬ì({user_email}) ìš”ì²­: ìºì‹œë¥¼ ê±´ë„ˆë›°ê³  ìµœê·¼ ì´ìŠˆë¥¼ ë‹¤ì‹œ ê²€ìƒ‰ ë° ìš”ì•½í•©ë‹ˆë‹¤.")
                    else:
                        logger.info(f"ì¢…ëª© [{stock_name}/{stock_code}]ì— ëŒ€í•œ ìºì‹œ ì—†ìŒ, ìµœê·¼ ì´ìŠˆ ìš”ì•½ ìƒì„±: {cache_key}")

                    recent_issues_summary = await self.summarize_recent_issues(stock_name, stock_code, user_id)

                    # ìƒì„±ëœ ìš”ì•½ì„ ìºì‹œì— ì €ì¥
                    expire_time = 259200  # 172800 2ì¼ -> 3ì¼
                    if settings.ENV == "development":
                        expire_time = 86400 * 7  # ê°œë°œë²„ì „ì€ 7ì¼ë‹¨ìœ„.
                    await redis_client.set_key(cache_key, recent_issues_summary, expire=expire_time)

                    log_message_prefix = "ê´€ë¦¬ì ìš”ì²­ìœ¼ë¡œ ê°±ì‹ ëœ " if is_admin_and_prod else ""
                    logger.info(f"{log_message_prefix}ìµœê·¼ ì´ìŠˆ ìš”ì•½ì„ ìºì‹œì— ì €ì¥ (ë§Œë£Œ: {expire_time}ì´ˆ): {cache_key}")

                final_report_toc = await self.generate_dynamic_toc(query, recent_issues_summary, user_id)
                return {"recent_issues_summary": recent_issues_summary, "final_report_toc": final_report_toc.model_dump()}

            # ë‘ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
            question_analysis_task = analyze_question_intent()
            issues_and_toc_task = search_issues_and_generate_toc()

            # ë³‘ë ¬ ì‘ì—… ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            question_analysis_result, issues_and_toc_result = await asyncio.gather(question_analysis_task, issues_and_toc_task)

            # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            state["question_analysis"] = question_analysis_result
            state["agent_results"]["question_analysis"] = question_analysis_result
            state["recent_issues_summary"] = issues_and_toc_result["recent_issues_summary"]
            state["final_report_toc"] = issues_and_toc_result["final_report_toc"]

            # ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # ë©”íŠ¸ë¦­ ê¸°ë¡
            state["metrics"] = state.get("metrics", {})
            state["metrics"]["question_analyzer"] = {
                "start_time": start_time,
                "end_time": end_time,
                "duration": duration,
                "status": "completed",
                "error": None,
                "model_name": self.agent_llm.get_model_name(),
            }

            # ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            state["processing_status"] = state.get("processing_status", {})
            state["processing_status"]["question_analyzer"] = "completed"

            logger.info(f"QuestionAnalyzerAgent completed in {duration:.2f} seconds")
            return state

        except Exception as e:
            logger.exception(f"Error in QuestionAnalyzerAgent: {str(e)}")
            self._add_error(state, f"ì§ˆë¬¸ ë¶„ì„ê¸° ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {str(e)}")
            return state

    async def analyze_conversation_context(
        self, query: str, conversation_history: List[Any], stock_name: str, stock_code: str, user_id: Optional[str] = None
    ) -> ConversationContextAnalysis:
        """
        í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì˜ì¡´í•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            query: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (LangChain ë©”ì‹œì§€ ê°ì²´ ëª©ë¡)

        Returns:
            ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
        """
        # logger.info(f"Analyzing conversation context dependency for query: {query}")

        if not conversation_history or len(conversation_history) < 2:
            # logger.info("ëŒ€í™” ê¸°ë¡ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ, ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê±´ë„ˆëœ€")
            return ConversationContextAnalysis(
                requires_context=False,
                is_followup_question=False,
                relation_to_previous="ë…ë¦½ì ",
                is_conversation_closing=False,
                closing_type=None,
                closing_response=None,
                is_different_stock=False,
                previous_stock_name=None,
                previous_stock_code=None,
                stock_relation="ì•Œìˆ˜ì—†ìŒ",
                reasoning="ëŒ€í™” ê¸°ë¡ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            )

        # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ… (ìµœê·¼ 3ë²ˆì˜ ëŒ€í™”ë§Œ ì‚¬ìš©)
        formatted_history = ""
        recent_history = conversation_history[-6:] if len(conversation_history) >= 6 else conversation_history

        for i, msg in enumerate(recent_history):
            role = "ì‚¬ìš©ì" if msg.type == "human" else "AI"
            formatted_history += f"{role}: {msg.content}\n\n"

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        system_prompt = """
ë‹¹ì‹ ì€ ëŒ€í™” íë¦„ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì˜ì¡´í•˜ëŠ”ì§€, ë…ë¦½ì ì¸ ìƒˆ ì§ˆë¬¸ì¸ì§€, ë˜ëŠ” ëŒ€í™” ë§ˆë¬´ë¦¬ë¥¼ ëœ»í•˜ëŠ” ì¸ì‚¬ë§ì¸ì§€ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì„¸ìš”:
1. ëŒ€ëª…ì‚¬(ì´ê²ƒ, ê·¸ê²ƒ, ì €ê²ƒ ë“±)ë‚˜ ìƒëµëœ ì£¼ì–´ê°€ ìˆëŠ”ì§€
2. ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ íŠ¹ì • ì •ë³´ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€
3. ì´ì „ ì‘ë‹µì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì¸ì§€
4. ì´ì „ ì‘ë‹µ ë‚´ìš©ì„ í™•ì¥í•˜ê±°ë‚˜ ìˆ˜ì •í•˜ë ¤ëŠ” ì˜ë„ê°€ ìˆëŠ”ì§€
5. "ê³ ë§ˆì›Œ", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì•Œê² ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ë„¤", "ë°”ë³´ì•¼" ë“± ëŒ€í™” ë§ˆë¬´ë¦¬ë¥¼ ëœ»í•˜ëŠ” í‘œí˜„ì¸ì§€
6. í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ì§ˆë¬¸ë“¤ì—ì„œ ì–¸ê¸‰ëœ ì¢…ëª©ê³¼ ë‹¤ë¥¸ ì¢…ëª©ì— ê´€í•œ ê²ƒì¸ì§€ íŒë‹¨í•˜ì„¸ìš”

ì˜ˆì‹œ)
 - ê³ ë§ˆì›Œ. ê°ì‚¬í•©ë‹ˆë‹¤ : ëŒ€í™” ë§ˆë¬´ë¦¬
 - ê³ ë§ˆì›Œ, ê·¸ëŸ¼ 24ë…„ ì˜ì—…ì´ìµì€ ì–´ë–»ê²Œ ë˜ëŠ”ê±°ì§€? : í›„ì† ì§ˆë¬¸
 - ì •ë³´ê°€ ì—†ë„¤. ë‹¤ë¥¸ ê²½ìŸì‚¬ë“¤ì€ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ ì°¾ì•„ë´ : í›„ì† ì§ˆë¬¸
 - ë°”ë³´ì•¼ : ëŒ€í™” ë§ˆë¬´ë¦¬

ì¢…ëª© ê´€ê³„ ë¶„ì„ ê°€ì´ë“œ:
- ì¢…ëª©ëª…ì´ë‚˜ ì¢…ëª©ì½”ë“œê°€ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê²½ìš°, ë§¥ë½ì„ í†µí•´ ë™ì¼ ì¢…ëª©ì— ê´€í•œ ì§ˆë¬¸ì¸ì§€ ì¶”ë¡ í•˜ì„¸ìš”.
- ì§ˆë¬¸ì—ì„œ ìƒˆë¡œìš´ ì¢…ëª©ì´ ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ ì´ì „ ì¢…ëª©ê³¼ì˜ ë¹„êµë¥¼ ìœ„í•œ ê²ƒì´ë¼ë©´, "ì¢…ëª©ë¹„êµ"ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- ì´ì „ ëŒ€í™”ì™€ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ìƒˆë¡œìš´ ì¢…ëª©ì— ëŒ€í•œ ì§ˆë¬¸ì´ë©´ "ë‹¤ë¥¸ì¢…ëª©"ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- ê°™ì€ ì¢…ëª©ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì´ë©´ "ë™ì¼ì¢…ëª©"ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.

ëŒ€í™” ë§ˆë¬´ë¦¬ë¡œ íŒë‹¨ëœ ê²½ìš°, ë§ˆë¬´ë¦¬ ìœ í˜•(ê¸ì •ì /ì¤‘ë¦½ì /ë¶€ì •ì )ì— ë”°ë¼ ì ì ˆí•œ ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”:
- ê¸ì •ì  ë§ˆë¬´ë¦¬(ì˜ˆ: "ê°ì‚¬í•©ë‹ˆë‹¤", "ê³ ë§ˆì›Œ"): ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ì—ˆë‹¤ëŠ” ë©”ì‹œì§€ë¡œ ì‘ë‹µ
- ì¤‘ë¦½ì  ë§ˆë¬´ë¦¬(ì˜ˆ: "ì•Œê² ìŠµë‹ˆë‹¤", "ëë‚´ì"): ê°„ê²°í•˜ê³  ì •ì¤‘í•œ ë§ˆë¬´ë¦¬ ë©”ì‹œì§€ë¡œ ì‘ë‹µ
- ë¶€ì •ì  ë§ˆë¬´ë¦¬(ì˜ˆ: "ì •ë³´ê°€ ì—†ë„¤", "ë°”ë³´ì•¼"): ê³µì†í•˜ê²Œ ì‚¬ê³¼í•˜ê³  ë” ë‚˜ì€ ì„œë¹„ìŠ¤ë¥¼ ì•½ì†í•˜ëŠ” ë©”ì‹œì§€ë¡œ ì‘ë‹µ

ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
- requires_context: ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œì§€ ì—¬ë¶€(true/false)
- is_followup_question: ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€(true/false)
- referenced_context: ì°¸ì¡°í•˜ëŠ” íŠ¹ì • ëŒ€í™” ë‚´ìš©(ìˆëŠ” ê²½ìš°)
- relation_to_previous: ì´ì „ ëŒ€í™”ì™€ì˜ ê´€ê³„("ë…ë¦½ì ", "ì§ì ‘ì°¸ì¡°", "ê°„ì ‘ì°¸ì¡°", "í™•ì¥", "ìˆ˜ì •" ì¤‘ í•˜ë‚˜)
- is_conversation_closing: ëŒ€í™” ë§ˆë¬´ë¦¬ë¥¼ ëœ»í•˜ëŠ” ì¸ì‚¬ë§ì¸ì§€ ì—¬ë¶€(true/false)
- closing_type: ë§ˆë¬´ë¦¬ ì¸ì‚¬ì˜ ìœ í˜•("ê¸ì •ì ", "ì¤‘ë¦½ì ", "ë¶€ì •ì " ì¤‘ í•˜ë‚˜, is_conversation_closingì´ trueì¸ ê²½ìš°ì—ë§Œ ê°’ ì œê³µ)
- closing_response: ë§ˆë¬´ë¦¬ ì¸ì‚¬ì— ëŒ€í•œ ì‘ë‹µ ë©”ì‹œì§€(is_conversation_closingì´ trueì¸ ê²½ìš°ì—ë§Œ ê°’ ì œê³µ)
- is_different_stock: ì´ì „ ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ ì¢…ëª©ì— ê´€í•œ ì§ˆë¬¸ì¸ì§€ ì—¬ë¶€(true/false)
- previous_stock_name: ì´ì „ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì¢…ëª©ëª…(ìˆëŠ” ê²½ìš°)
- previous_stock_code: ì´ì „ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ì¢…ëª©ì½”ë“œ(ìˆëŠ” ê²½ìš°)
- stock_relation: ì´ì „ ì¢…ëª©ê³¼ì˜ ê´€ê³„("ë™ì¼ì¢…ëª©", "ì¢…ëª©ë¹„êµ", "ë‹¤ë¥¸ì¢…ëª©", "ì•Œìˆ˜ì—†ìŒ" ì¤‘ í•˜ë‚˜)
- reasoning: íŒë‹¨ ì´ìœ  ì„¤ëª…(3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ)
"""

        user_prompt = f"""
ëŒ€í™” ê¸°ë¡:
{formatted_history}

í˜„ì¬ ì§ˆë¬¸:
{query}

í˜„ì¬ ì¢…ëª© ì •ë³´: [ì¢…ëª©ëª…: {stock_name}, ì¢…ëª©ì½”ë“œ: {stock_code}]

ìœ„ ëŒ€í™”ì—ì„œ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì— ì˜ì¡´í•˜ëŠ”ì§€, ëŒ€í™” ë§ˆë¬´ë¦¬ í‘œí˜„ì¸ì§€, ì¢…ëª© ê´€ê³„ëŠ” ì–´ë–¤ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""

        try:
            [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]
            formatted_prompt = f"{system_prompt}\n\n{user_prompt}"
            # LLM í˜¸ì¶œë¡œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            response = await self.agent_llm_lite.with_structured_output(ConversationContextAnalysis).ainvoke(
                formatted_prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db
            )

            logger.info(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼: {response}")
            return response

        except Exception as e:
            logger.error(f"ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return ConversationContextAnalysis(
                requires_context=False,
                is_followup_question=False,
                relation_to_previous="ë…ë¦½ì ",
                is_conversation_closing=False,
                closing_type=None,
                closing_response=None,
                is_different_stock=False,
                previous_stock_name=None,
                previous_stock_code=None,
                stock_relation="ì•Œìˆ˜ì—†ìŒ",
                reasoning=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            )

    def _add_error(self, state: Dict[str, Any], error_message: str) -> None:
        """
        ìƒíƒœ ê°ì²´ì— ì˜¤ë¥˜ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            state: ìƒíƒœ ê°ì²´
            error_message: ì˜¤ë¥˜ ë©”ì‹œì§€
        """
        state["errors"] = state.get("errors", [])
        state["errors"].append(
            {"agent": "question_analyzer", "error": error_message, "type": "processing_error", "timestamp": datetime.now(), "context": {"query": state.get("query", "")}}
        )

        # ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        state["processing_status"] = state.get("processing_status", {})
        state["processing_status"]["question_analyzer"] = "failed"

    # ë™ì  ëª©ì°¨ ìƒì„± í•¨ìˆ˜ ì¶”ê°€
    async def generate_dynamic_toc(self, query: str, recent_issues_summary: str, user_id: str) -> DynamicTocOutput:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìµœê·¼ ì´ìŠˆ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ë™ì ì¸ ëª©ì°¨ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

        Args:
            query (str): ì‚¬ìš©ìì˜ ì´ˆê¸° ì§ˆë¬¸
            recent_issues_summary (str): ìµœê·¼ ì´ìŠˆ ìš”ì•½

        Returns:
            DynamicTocOutput: ìƒì„±ëœ ëª©ì°¨ êµ¬ì¡°
        """
        logger.info("\nğŸ“‹ ë™ì  ëª©ì°¨ ìƒì„± ì¤‘...")

        prompt_template = ChatPromptTemplate.from_template(PROMPT_DYNAMIC_TOC).partial(
            query=query, recent_issues_summary=recent_issues_summary, today_date=datetime.now().strftime("%Y-%m-%d")
        )
        formatted_prompt = prompt_template.format_prompt()

        # 1. ë¨¼ì € êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì‹œë„
        try:
            logger.info("êµ¬ì¡°í™”ëœ ì¶œë ¥(DynamicTocOutput)ì„ ì‚¬ìš©í•˜ì—¬ ëª©ì°¨ ìƒì„± ì‹œë„")
            structured_response = await self.agent_llm.with_structured_output(DynamicTocOutput).ainvoke(
                formatted_prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db
            )

            # êµ¬ì¡°í™”ëœ ì¶œë ¥ì´ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ê²½ìš°
            logger.info(f"\nâœ… êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„±ê³µ: title={structured_response.title}, sections={len(structured_response.sections)}ê°œ")

            # ì„¹ì…˜ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° í™•ì¸
            if len(structured_response.sections) == 0:
                logger.warning("êµ¬ì¡°í™”ëœ ì¶œë ¥ì— ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°ë¡œ fallbackí•©ë‹ˆë‹¤.")
                raise ValueError("êµ¬ì¡°í™”ëœ ì¶œë ¥ì— ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")

            return structured_response

        except Exception as e:
            # êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallbackìœ¼ë¡œ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì‹œë„
            logger.warning(f"\nâš ï¸ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‹¤íŒ¨: {str(e)}, ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µìœ¼ë¡œ fallback")

            # 2. ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì‹œë„
            response: AIMessage = await self.agent_llm.ainvoke_with_fallback(formatted_prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db)

            response_text = response.content
            logger.info("\nğŸ“„ LLM ì›ë³¸ ì‘ë‹µ:")
            logger.info(f"\n{response_text[:200]}")  # ì‘ë‹µ ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)

            # 3. JSON ë¬¸ìì—´ íŒŒì‹± ì‹œë„
            try:
                # JSON ë¶€ë¶„ ì¶”ì¶œ (LLMì´ JSON ì™¸ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ)
                json_str = extract_json_from_text(response_text)
                if json_str:
                    # JSON ë¬¸ìì—´ íŒŒì‹±
                    toc_data = json.loads(json_str)
                    logger.info(f"\nâœ… JSON íŒŒì‹± ì„±ê³µ: title={toc_data.get('title')}, sections={len(toc_data.get('sections', []))}ê°œ")

                    # ì„¹ì…˜ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° í™•ì¸
                    if len(toc_data.get("sections", [])) == 0:
                        logger.warning("JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª©ì°¨ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        raise ValueError("JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")

                    # JSON ë°ì´í„°ë¥¼ DynamicTocOutput ëª¨ë¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    converted_sections = []
                    for section in toc_data.get("sections", []):
                        # ì„œë¸Œì„¹ì…˜ ë³€í™˜
                        converted_subsections = []
                        for subsection in section.get("subsections", []):
                            converted_subsections.append(
                                SubsectionModel(subsection_id=subsection.get("subsection_id", ""), title=subsection.get("title", ""), description=subsection.get("description"))
                            )

                        # ì„¹ì…˜ ë³€í™˜
                        converted_sections.append(
                            SectionModel(
                                section_id=section.get("section_id", ""), title=section.get("title", ""), description=section.get("description"), subsections=converted_subsections
                            )
                        )

                    # DynamicTocOutput ê°ì²´ ìƒì„±
                    result = DynamicTocOutput(title=toc_data.get("title", f"íˆ¬ì ë¦¬ì„œì¹˜ ë³´ê³ ì„œ: {query}"), sections=converted_sections)

                    return result

                else:
                    # JSON íŒ¨í„´ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    logger.warning("JSON ë¬¸ìì—´ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ì‚¬ìš©")
                    raise ValueError("JSON ë¬¸ìì—´ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as json_error:
                # 4. JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª©ì°¨ êµ¬ì¡° ì‚¬ìš©
                logger.warning(f"\nâš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {str(json_error)}, ê¸°ë³¸ ëª©ì°¨ êµ¬ì¡° ì‚¬ìš©")
                logger.warning(f"LLM ì›ë³¸ ì‘ë‹µ:\n{response_text}")

                # ê¸°ë³¸ ëª©ì°¨ êµ¬ì¡° ìƒì„±
                default_sections = [
                    SectionModel(section_id="section_1", title="í•µì‹¬ ìš”ì•½ (Executive Summary)", description="ì£¼ìš” ë°œê²¬ê³¼ ê²°ë¡ ì„ ìš”ì•½", subsections=[]),
                    SectionModel(section_id="section_2", title="ê¸°ì—… ê°œìš” ë° ì‚¬ì—… ëª¨ë¸", description="ê¸°ì—…ì˜ ê¸°ë³¸ ì •ë³´ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„", subsections=[]),
                    SectionModel(section_id="section_3", title="ì‚°ì—…/ì‹œì¥ ë™í–¥ ë¶„ì„", description="ê¸°ì—…ì´ ì†í•œ ì‚°ì—…ì˜ í˜„í™©ê³¼ ì „ë§", subsections=[]),
                ]

                # ê¸°ë³¸ DynamicTocOutput ê°ì²´ ìƒì„±
                result = DynamicTocOutput(title=f"íˆ¬ì ë¦¬ì„œì¹˜ ë³´ê³ ì„œ: {query}", sections=default_sections)

                print(f"\nâœ… ë™ì  ëª©ì°¨ ìƒì„± ì™„ë£Œ. ì´ {len(result.sections)}ê°œ ì„¹ì…˜ í¬í•¨")
                print(f"ğŸ“š ë³´ê³ ì„œ ì œëª©: {result.title}")

                # ì„¹ì…˜ ì •ë³´ ìƒì„¸ ì¶œë ¥
                print("ğŸ“‘ ëª©ì°¨ êµ¬ì¡°:")
                for section in result.sections:
                    print(f"  {section.title}")
                    if section.description:
                        print(f"     - {section.description}")

                    # í•˜ìœ„ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ì¶œë ¥
                    if section.subsections:
                        for subsection in section.subsections:
                            print(f"     {subsection.title}")

                return result

    async def summarize_recent_issues(self, stock_name: str, stock_code: str, user_id: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ìµœê·¼ ì´ìŠˆ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""

        search_results = await self.search_recent_issues(stock_name, stock_code)  # ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰

        print(f"\nğŸ“ {stock_name}ì˜ ìµœê·¼ ì´ìŠˆ ìš”ì•½ ì¤‘...")
        prompt = f"""
    ë‹¤ìŒì€ '{stock_name}'ì— ëŒ€í•œ ìµœê·¼ ì£¼ìš” ë‰´ìŠ¤ ë° ì´ìŠˆ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ìš” ë‰´ìŠ¤ ì œëª©, í•µì‹¬ ì´ìŠˆ, ë°˜ë³µì ìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ìš”ì•½ì€ ê¸€ë¨¸ë¦¬ ê¸°í˜¸(ë¶ˆë¦¿ í¬ì¸íŠ¸) í˜•ì‹ì„ ì‚¬ìš©í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•œ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”..

    ê²€ìƒ‰ ê²°ê³¼:
    {search_results}

    ìµœê·¼ ì£¼ìš” ë‰´ìŠ¤ ë° ì´ìŠˆ ê²€ìƒ‰ ê²°ê³¼ í‚¤ì›Œë“œ ìš”ì•½:
    """
        try:
            response = await self.agent_llm_lite.ainvoke_with_fallback(prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db)

            summary = response.content
            print(f"  ğŸ“ {stock_name} ìµœê·¼ ì´ìŠˆ ìš”ì•½ ì™„ë£Œ.")
            # print(f"=== ìš”ì•½ ë‚´ìš© ===\\n{summary}\\n===========") # ë””ë²„ê¹…ìš©
            return summary
        except Exception as e:
            print(f"  âš ï¸ {stock_name} ìµœê·¼ ì´ìŠˆ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"{stock_name} ìµœê·¼ ì´ìŠˆ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def search_recent_issues(self, stock_name: str, stock_code: str) -> str:
        """Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì¢…ëª©ì˜ ìµœê·¼ 6ê°œì›”ê°„ ì£¼ìš” ë‰´ìŠ¤ ë° ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” {stock_name}ì˜ ìµœê·¼ ì£¼ìš” ì´ìŠˆ ê²€ìƒ‰ ì¤‘...")
        query = f"{stock_name}({stock_code}) ìµœê·¼ ì£¼ìš” ë‰´ìŠ¤ ë° í•µì‹¬ ì´ìŠˆ ë™í–¥"
        try:
            # search_with_tavily í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ Tavily í˜¸ì¶œ ë¡œì§ êµ¬í˜„
            search_results = await self.search_with_tavily(query)
            print(f"  ğŸ“Š {stock_name} ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ ì™„ë£Œ.\n[{search_results[:200]}]")

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
            await self._save_recent_issues_to_json(stock_name, stock_code, query, search_results)

            return search_results
        except Exception as e:
            print(f"  âš ï¸ {stock_name} ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"{stock_name} ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    async def _save_recent_issues_to_json(self, stock_name: str, stock_code: str, query: str, search_results: Any) -> None:
        """
        ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¼ìë³„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

        Args:
            stock_name: ì¢…ëª© ì´ë¦„
            stock_code: ì¢…ëª© ì½”ë“œ
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_results: Tavily API ê²€ìƒ‰ ê²°ê³¼

        Returns:
            None
        """
        try:
            # íŒŒì¼ I/O ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì •ì˜
            def write_to_json() -> str:
                # JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
                json_dir = os.path.join("stockeasy", "local_cache", "web_search")
                os.makedirs(json_dir, exist_ok=True)

                date_str = datetime.now().strftime("%Y%m%d")
                json_path = os.path.join(json_dir, f"recent_issues_{date_str}.json")

                # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„
                current_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
                entry = {"timestamp": current_datetime, "stock_code": stock_code, "stock_name": stock_name, "query": query, "search_results": search_results}

                # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                data = []
                if os.path.exists(json_path) and os.path.getsize(json_path) > 0:
                    try:
                        with open(json_path, "r", encoding="utf-8-sig") as json_file:
                            data = json.load(json_file)
                    except json.JSONDecodeError:
                        # íŒŒì¼ì´ ì†ìƒëœ ê²½ìš° ìƒˆë¡œ ì‹œì‘
                        data = []

                # ë°ì´í„° ì¶”ê°€
                data.append(entry)

                # íŒŒì¼ì— ì €ì¥
                with open(json_path, "w", encoding="utf-8-sig") as json_file:
                    json.dump(data, json_file, ensure_ascii=False, indent=2)

                return json_path

            # íŒŒì¼ I/O ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            json_path = await asyncio.to_thread(write_to_json)

            print(f"  ğŸ’¾ {stock_name} ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ê²°ê³¼ê°€ JSON íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {json_path}")

        except Exception as e:
            print(f"  âš ï¸ JSON íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    async def search_with_tavily(self, query: str) -> str:
        """Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # search_results = await self.tavily_search.ainvoke({"query": query,
            #                                                    "search_depth":"basic",# "advanced",
            #                                                 "max_results": 10,
            #                                                 "topic": "general",
            #                                                 #"topic":"finance",
            #                                                 "time_range" : "6m",
            #                                                 "chunks_per_source": 3,
            #                                                 "include_raw_content": True,
            #                                                 "include_answer":True
            #                                                 })
            # search_results = await self.tavily_search.ainvoke({"query": query,
            #                                     "search_depth": "advanced", # "basic",
            #                                     #"search_depth": "basic", # "basic",
            #                                     "max_results": 14,
            #                                     "topic": "general",
            #                                     #"topic":"finance",
            #                                     "time_range" : "year",
            #                                     })
            search_results = await self.tavily_service.search_async(
                query=query,
                search_depth="advanced",  # "basic",
                # "search_depth": "basic", # "basic",
                max_results=14,
                topic="general",
                # "topic":"finance",
                time_range="year",
            )

            # print(f"ê²€ìƒ‰ê²°ê³¼ : {search_results}")
            # print(f"ê²€ìƒ‰ê²°ê³¼ ì‹œê°„ : {search_results.get('response_time', '0')}")
            # print(f"ê²€ìƒ‰ê²°ê³¼ ì‘ë‹µ : {search_results.get('answer', 'None')}")
            formatted_results = "ê²€ìƒ‰ ê²°ê³¼:\n\n"
            for i, result_item in enumerate(search_results.get("results", []), 1):
                # result_itemì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸ í›„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                if isinstance(result_item, dict):
                    formatted_results += f"{i}. ì œëª©: {result_item.get('title', 'ì œëª© ì—†ìŒ')}\n"
                    formatted_results += f"   URL: {result_item.get('url', 'ë§í¬ ì—†ìŒ')}\n"
                    formatted_results += f"   ë‚´ìš©: {result_item.get('content', 'ë‚´ìš© ì—†ìŒ')}\n\n"
                else:
                    # result_itemì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ë¡œê·¸ë¥¼ ë‚¨ê¸°ê±°ë‚˜ ë‹¤ë¥¸ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì´ ì˜ˆìƒëœ ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤: {result_item}")
                    formatted_results += f"{i}. ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ê²°ê³¼ í•­ëª©ì…ë‹ˆë‹¤.\n\n"
            return formatted_results
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _detect_technical_analysis_need(self, query: str) -> bool:
        """
        ì§ˆë¬¸ì—ì„œ ê¸°ìˆ ì  ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ê°ì§€í•˜ì—¬ ê¸°ìˆ ì  ë¶„ì„ í•„ìš”ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ê¸°ìˆ ì  ë¶„ì„ì´ í•„ìš”í•œì§€ ì—¬ë¶€
        """
        try:
            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ë¶„ì„ ì‹œì‘ - ì¿¼ë¦¬: '{query}'")

            # ì§ˆë¬¸ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì ë¬´ê´€í•˜ê²Œ ê²€ìƒ‰
            query_lower = query.lower()
            logger.debug(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ì†Œë¬¸ì ë³€í™˜: '{query_lower}'")

            # ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            keyword_scores = {}
            total_matches = 0

            for category, keywords in self.technical_analysis_keywords.items():
                matches = 0
                matched_keywords = []

                for keyword in keywords:
                    keyword_lower = keyword.lower()
                    if keyword_lower in query_lower:
                        matches += 1
                        matched_keywords.append(keyword)
                        total_matches += 1
                        logger.debug(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] í‚¤ì›Œë“œ ë§¤ì¹­: '{keyword}' in '{query_lower}' (ì¹´í…Œê³ ë¦¬: {category})")

                keyword_scores[category] = {"matches": matches, "matched_keywords": matched_keywords, "score": matches / len(keywords) if keywords else 0}

                if matches > 0:
                    logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] {category} ì¹´í…Œê³ ë¦¬ ë§¤ì¹­: {matches}ê°œ - {matched_keywords}")

            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ì „ì²´ í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼: ì´ {total_matches}ê°œ ë§¤ì¹­")
            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜: {keyword_scores}")

            # ê¸°ìˆ ì  ë¶„ì„ í•„ìš”ì„± íŒë‹¨ ë¡œì§
            needs_technical_analysis = False
            reasoning = []

            # 1. ì§ì ‘ì ì¸ ê¸°ìˆ ë¶„ì„ í‚¤ì›Œë“œ í™•ì¸
            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ê·œì¹™1 í™•ì¸ - market_analysis ë§¤ì¹­: {keyword_scores['market_analysis']['matches']}ê°œ")
            if keyword_scores["market_analysis"]["matches"] > 0:
                needs_technical_analysis = True
                reasoning.append(f"ê¸°ìˆ ë¶„ì„ ì§ì ‘ í‚¤ì›Œë“œ ê°ì§€: {keyword_scores['market_analysis']['matched_keywords']}")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âœ… ê·œì¹™1 í†µê³¼ - ê¸°ìˆ ë¶„ì„ ì§ì ‘ í‚¤ì›Œë“œ ê°ì§€")

            # 2. ê¸°ìˆ ì  ì§€í‘œ í‚¤ì›Œë“œ í™•ì¸ (2ê°œ ì´ìƒì´ë©´ ë†’ì€ í™•ë¥ )
            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ê·œì¹™2 í™•ì¸ - technical_indicators ë§¤ì¹­: {keyword_scores['technical_indicators']['matches']}ê°œ")
            if keyword_scores["technical_indicators"]["matches"] >= 2:
                needs_technical_analysis = True
                reasoning.append(f"ê¸°ìˆ ì  ì§€í‘œ í‚¤ì›Œë“œ ë‹¤ì¤‘ ê°ì§€: {keyword_scores['technical_indicators']['matched_keywords']}")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âœ… ê·œì¹™2a í†µê³¼ - ê¸°ìˆ ì  ì§€í‘œ í‚¤ì›Œë“œ ë‹¤ì¤‘ ê°ì§€")
            elif keyword_scores["technical_indicators"]["matches"] >= 1:
                # 1ê°œë¼ë„ ìˆìœ¼ë©´ ì¼ë‹¨ í›„ë³´ë¡œ ê³ ë ¤
                reasoning.append(f"ê¸°ìˆ ì  ì§€í‘œ í‚¤ì›Œë“œ ê°ì§€: {keyword_scores['technical_indicators']['matched_keywords']}")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ğŸ“ ê·œì¹™2b - ê¸°ìˆ ì  ì§€í‘œ í‚¤ì›Œë“œ 1ê°œ ê°ì§€ (í›„ë³´)")

            # 3. ë§¤ë§¤ ì‹ í˜¸ í‚¤ì›Œë“œ í™•ì¸
            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ê·œì¹™3 í™•ì¸ - trading_signals ë§¤ì¹­: {keyword_scores['trading_signals']['matches']}ê°œ")
            if keyword_scores["trading_signals"]["matches"] >= 1:
                needs_technical_analysis = True
                reasoning.append(f"ë§¤ë§¤ ì‹ í˜¸ í‚¤ì›Œë“œ ê°ì§€: {keyword_scores['trading_signals']['matched_keywords']}")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âœ… ê·œì¹™3 í†µê³¼ - ë§¤ë§¤ ì‹ í˜¸ í‚¤ì›Œë“œ ê°ì§€")

            # 4. ì°¨íŠ¸ íŒ¨í„´ ê¸°ë°˜ íŒë‹¨ (íŒ¨í„´ í‚¤ì›Œë“œ 1ê°œë§Œ ìˆì–´ë„ ê¸°ìˆ ì  ë¶„ì„ìœ¼ë¡œ ë¶„ë¥˜)
            chart_pattern_matches = keyword_scores["chart_patterns"]["matches"]
            technical_indicator_matches = keyword_scores["technical_indicators"]["matches"]
            price_movement_matches = keyword_scores["price_movements"]["matches"]
            logger.info(
                f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ê·œì¹™4 í™•ì¸ - chart_patterns: {chart_pattern_matches}ê°œ, technical_indicators: {technical_indicator_matches}ê°œ, price_movements: {price_movement_matches}ê°œ"
            )

            if chart_pattern_matches >= 1:
                needs_technical_analysis = True
                reasoning.append(f"ì°¨íŠ¸íŒ¨í„´ í‚¤ì›Œë“œ ê°ì§€: {keyword_scores['chart_patterns']['matched_keywords']}")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âœ… ê·œì¹™4a í†µê³¼ - ì°¨íŠ¸íŒ¨í„´ í‚¤ì›Œë“œ ê°ì§€ (1ê°œ ì´ìƒ)")
            elif price_movement_matches >= 2:
                # ê°€ê²© ì›€ì§ì„ë§Œìœ¼ë¡œëŠ” ì•½í•˜ì§€ë§Œ 2ê°œ ì´ìƒì´ë©´ ê³ ë ¤
                reasoning.append(f"ê°€ê²© ì›€ì§ì„ í‚¤ì›Œë“œ ë‹¤ì¤‘ ê°ì§€: {keyword_scores['price_movements']['matched_keywords']}")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ğŸ“ ê·œì¹™4b - ê°€ê²© ì›€ì§ì„ í‚¤ì›Œë“œ ë‹¤ì¤‘ ê°ì§€ (í›„ë³´)")

            # 5. ì „ì²´ ë§¤ì¹­ í‚¤ì›Œë“œ ìˆ˜ê°€ ë§ìœ¼ë©´ ê¸°ìˆ ì  ë¶„ì„ ê°€ëŠ¥ì„± ë†’ìŒ
            logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ê·œì¹™5 í™•ì¸ - ì´ ë§¤ì¹­ í‚¤ì›Œë“œ: {total_matches}ê°œ, í˜„ì¬ ê²°ê³¼: {needs_technical_analysis}")
            if total_matches >= 2 and not needs_technical_analysis:
                needs_technical_analysis = True
                reasoning.append(f"ê¸°ìˆ ì  ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œ ë‹¤ìˆ˜ ê°ì§€ (ì´ {total_matches}ê°œ)")
                logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âœ… ê·œì¹™5 í†µê³¼ - í‚¤ì›Œë“œ ë‹¤ìˆ˜ ê°ì§€ (2ê°œ ì´ìƒ)")

            # ìµœì¢… íŒë‹¨ ë¡œê¹…
            if needs_technical_analysis:
                logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ğŸ¯ ìµœì¢… ê²°ê³¼: TRUE - ì´ìœ : {', '.join(reasoning)}")
            else:
                logger.info(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âŒ ìµœì¢… ê²°ê³¼: FALSE - ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì´ {total_matches}ê°œ")

            return needs_technical_analysis

        except Exception as e:
            logger.error(f"[ê¸°ìˆ ì ë¶„ì„ê°ì§€] âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.info("[ê¸°ìˆ ì ë¶„ì„ê°ì§€] ğŸ›¡ï¸ ì•ˆì „í•˜ê²Œ False ë°˜í™˜")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ False ë°˜í™˜
            return False

    async def _handle_general_question(self, state: Dict[str, Any], query: str, start_time: datetime) -> Dict[str, Any]:
        """
        ì¼ë°˜ ì§ˆë¬¸ ëª¨ë“œ ì²˜ë¦¬

        Args:
            state: í˜„ì¬ ìƒíƒœ
            query: ì‚¬ìš©ì ì§ˆë¬¸
            start_time: ì²˜ë¦¬ ì‹œì‘ ì‹œê°„

        Returns:
            ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
        """
        try:
            logger.info("ì¼ë°˜ ì§ˆë¬¸ ë¶„ì„ ì‹œì‘")

            # user_id ì¶”ì¶œ
            user_context = state.get("user_context", {})
            user_id = user_context.get("user_id", None)

            # 1. ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¢…ëª© ì •ë³´ ì¶”ì¶œ ì‹œë„
            extracted_entities = await self._extract_entities_from_query(query, user_id)
            stock_name = extracted_entities.get("stock_name")
            stock_code = extracted_entities.get("stock_code")
            has_stock_reference = extracted_entities.get("has_stock_reference", False)

            # 2. ì§ˆë¬¸ ë¶„ì„
            analysis_result = await self._analyze_general_question(query, extracted_entities, state)

            # 3. ìµœê·¼ ì´ìŠˆ ìš”ì•½ (ì¢…ëª©ì´ ì–¸ê¸‰ëœ ê²½ìš°)
            recent_issues_summary = ""
            if has_stock_reference and stock_name and stock_code:
                logger.info(f"ì¼ë°˜ ì§ˆë¬¸ ë‚´ ì¢…ëª©({stock_name}) ì–¸ê¸‰ í™•ì¸, ìµœê·¼ ì´ìŠˆ ê²€ìƒ‰ ìˆ˜í–‰")
                recent_issues_summary = await self.summarize_recent_issues(stock_name, stock_code, user_id)

            # 4. ê¸°ìˆ ì  ë¶„ì„ ì„¹ì…˜ ì œì™¸í•œ ë™ì  ëª©ì°¨ ìƒì„±
            # ì°¸ê³ : _generate_general_tocëŠ” analysis_resultê°€ í•„ìš”í•˜ì—¬ ë³‘ë ¬ì²˜ë¦¬ ë¶ˆê°€
            final_report_toc = await self._generate_general_toc(query, analysis_result, user_id)

            # 5. ê²°ê³¼ ì €ì¥ (stock-specific í”Œë¡œìš°ì™€ ë™ì¼í•œ êµ¬ì¡°ë¡œ)
            state["question_analysis"] = analysis_result
            state["agent_results"] = state.get("agent_results", {})
            state["agent_results"]["question_analysis"] = analysis_result
            state["recent_issues_summary"] = recent_issues_summary
            state["final_report_toc"] = final_report_toc

            # ë©”íŠ¸ë¦­ ê¸°ë¡
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            state["metrics"] = state.get("metrics", {})
            state["metrics"]["question_analyzer"] = {
                "start_time": start_time,
                "end_time": end_time,
                "duration": duration,
                "status": "completed",
                "error": None,
                "model_name": self.agent_llm.get_model_name(),
                "general_mode": True,
            }

            state["processing_status"] = state.get("processing_status", {})
            state["processing_status"]["question_analyzer"] = "completed"

            logger.info(f"ì¼ë°˜ ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: {duration:.2f}ì´ˆ ì†Œìš”")
            return state

        except Exception as e:
            logger.error(f"ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            self._add_error(state, f"ì¼ë°˜ ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return state

    async def _extract_entities_from_query(self, query: str, user_id: Optional[str] = None) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¢…ëª© ì •ë³´ ì¶”ì¶œ"""
        try:
            extraction_prompt = f"""
            ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì¢…ëª©ëª…, ì¢…ëª©ì½”ë“œ, ì„¹í„° ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

            ì§ˆë¬¸: {query}

            - subgroupì€ ì‚¬ìš©ì ì§ˆë¬¸ì— í¬í•¨ëœ í‚¤ì›Œë“œ í˜¹ì€ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì˜ë„í•˜ëŠ” í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼í•©ë‹ˆë‹¤.
            - subgroupì€ ìµœì†Œ 1ê°œ ì´ìƒì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼í•©ë‹ˆë‹¤.

            JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
            {{
                "stock_name": "ì¢…ëª©ëª… ë˜ëŠ” null",
                "stock_code": "ì¢…ëª©ì½”ë“œ ë˜ëŠ” null", 
                "sector": "ì„¹í„° ë˜ëŠ” null",
                "subgroup": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
                "has_stock_reference": True/False,
            }}
            """

            response = await self.agent_llm_lite.ainvoke_with_fallback(extraction_prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db)

            # JSON ì‘ë‹µ íŒŒì‹±
            content = response.content if hasattr(response, "content") else str(response)
            json_str = remove_json_block(content)

            if json_str:
                try:
                    result = json.loads(json_str)
                except json.JSONDecodeError:
                    result = {"has_stock_reference": False}
            else:
                result = {"has_stock_reference": False}

            return result if result else {"has_stock_reference": False}

        except Exception as e:
            logger.error(f"ì—”í‹°í‹° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {"has_stock_reference": False}

    async def _analyze_general_question(self, query: str, entities: Dict[str, Any], state: Dict[str, Any]) -> Dict[str, Any]:
        """ì¼ë°˜ ì§ˆë¬¸ ë¶„ì„"""
        try:
            # ê¸°ìˆ ì  ë¶„ì„ í•„ìš”ì„± ì„¤ì • (ê¸°ë³¸ê°’ê³¼ ë™ì¼)
            ta_needed_default = True
            logger.info(f"[ì¼ë°˜ì§ˆë¬¸ë¶„ì„] ê¸°ìˆ ì ë¶„ì„ í•„ìš”ì„± ì„¤ì •: {ta_needed_default} (ë¬´ì¡°ê±´ í™œì„±í™”)")

            stock_name = entities.get("stock_name")
            stock_code = entities.get("stock_code")
            subgroup = entities.get("subgroup")

            # create_default_question_analysisì™€ ì™„ì „íˆ ë™ì¼í•œ êµ¬ì¡°
            analysis_result = {
                "entities": {
                    "stock_name": stock_name,
                    "stock_code": stock_code,
                    "sector": None,
                    "subgroup": subgroup if subgroup else [],  # None ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    "time_range": None,
                    "financial_metric": None,
                    "competitor": None,
                    "product": None,
                },
                "classification": {"primary_intent": "ì¢…ëª©ê¸°ë³¸ì •ë³´", "complexity": "ì¤‘ê°„", "expected_answer_type": "ì‚¬ì‹¤í˜•"},
                "data_requirements": {
                    "telegram_needed": True,
                    "reports_needed": True,
                    "financial_statements_needed": True,
                    "industry_data_needed": True,
                    "confidential_data_needed": True,
                    "revenue_data_needed": True,
                    "web_search_needed": False,
                    "technical_analysis_needed": ta_needed_default,
                },
                "keywords": [stock_name or "ì¼ë°˜ì§ˆë¬¸", "ì •ë³´"],
                "detail_level": "ë³´í†µ",
            }

            return analysis_result

        except Exception as e:
            logger.error(f"ì¼ë°˜ ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            raise

    async def _generate_general_toc(self, query: str, analysis_result: Dict[str, Any], user_id: Optional[str] = None) -> Dict[str, Any]:
        """ì¼ë°˜ ì§ˆë¬¸ìš© ë™ì  ëª©ì°¨ ìƒì„± (ê¸°ìˆ ì  ë¶„ì„ ì„¹ì…˜ ì œì™¸)"""
        try:
            logger.info("\nğŸ“‹ ì¼ë°˜ ì§ˆë¬¸ ë™ì  ëª©ì°¨ ìƒì„± ì¤‘...")

            prompt_template = ChatPromptTemplate.from_template(PROMPT_DYNAMIC_GENERAL_TOC).partial(
                query=query, analysis_result=analysis_result, today_date=datetime.now().strftime("%Y-%m-%d")
            )
            formatted_prompt = prompt_template.format_prompt()

            # 1. ë¨¼ì € êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì‹œë„
            try:
                logger.info("êµ¬ì¡°í™”ëœ ì¶œë ¥(DynamicTocOutput)ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ ëª©ì°¨ ìƒì„± ì‹œë„")
                structured_response = await self.agent_llm.with_structured_output(DynamicTocOutput).ainvoke(
                    formatted_prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db
                )

                # êµ¬ì¡°í™”ëœ ì¶œë ¥ì´ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ê²½ìš°
                logger.info(f"\nâœ… êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„±ê³µ: title={structured_response.title}, sections={len(structured_response.sections)}ê°œ")

                # ì„¹ì…˜ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° í™•ì¸
                if len(structured_response.sections) == 0:
                    logger.warning("êµ¬ì¡°í™”ëœ ì¶œë ¥ì— ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡°ë¡œ fallbackí•©ë‹ˆë‹¤.")
                    raise ValueError("êµ¬ì¡°í™”ëœ ì¶œë ¥ì— ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")

                # DynamicTocOutputì„ Dictë¡œ ë³€í™˜
                return {
                    "title": structured_response.title,
                    "sections": [
                        {
                            "section_id": section.section_id,
                            "title": section.title,
                            "description": section.description,
                            "subsections": [
                                {"subsection_id": subsection.subsection_id, "title": subsection.title, "description": subsection.description} for subsection in section.subsections
                            ],
                        }
                        for section in structured_response.sections
                    ],
                }

            except Exception as e:
                # êµ¬ì¡°í™”ëœ ì¶œë ¥ íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallbackìœ¼ë¡œ ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì‹œë„
                logger.warning(f"\nâš ï¸ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‹¤íŒ¨: {str(e)}, ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µìœ¼ë¡œ fallback")

                # 2. ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ ì‹œë„
                response = await self.agent_llm.ainvoke_with_fallback(formatted_prompt, project_type=ProjectType.STOCKEASY, user_id=user_id, db=self.db)

                # JSON ì‘ë‹µ íŒŒì‹±
                content = response.content if hasattr(response, "content") else str(response)
                json_str = extract_json_from_text(content)

                if json_str:
                    try:
                        result = json.loads(json_str)
                        logger.info(f"\nâœ… JSON íŒŒì‹± ì„±ê³µ: title={result.get('title')}, sections={len(result.get('sections', []))}ê°œ")

                        # ì„¹ì…˜ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° í™•ì¸
                        if len(result.get("sections", [])) == 0:
                            logger.warning("JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª©ì°¨ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            raise ValueError("JSON íŒŒì‹± ì„±ê³µí–ˆìœ¼ë‚˜ ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")

                        return result
                    except json.JSONDecodeError as parse_error:
                        logger.error(f"ì¼ë°˜ ëª©ì°¨ JSON íŒŒì‹± ì˜¤ë¥˜: {parse_error}")
                        raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨")
                else:
                    logger.warning("JSON ë¬¸ìì—´ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ì‚¬ìš©")
                    raise ValueError("JSON ë¬¸ìì—´ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"ì¼ë°˜ ì§ˆë¬¸ ëª©ì°¨ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            # ê¸°ë³¸ ëª©ì°¨ ë°˜í™˜
            return {
                "title": "ì¼ë°˜ ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼",
                "sections": [
                    {"section_id": "section_1", "title": "1. í•µì‹¬ ìš”ì•½", "description": "ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‚´ìš© ìš”ì•½", "subsections": []},
                    {"section_id": "section_2", "title": "2. ìƒì„¸ ë¶„ì„", "description": "ì§ˆë¬¸ ì£¼ì œì— ëŒ€í•œ ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©", "subsections": []},
                    {"section_id": "section_3", "title": "3. ê²°ë¡  ë° ì‹œì‚¬ì ", "description": "ë¶„ì„ ê²°ê³¼ ë° í–¥í›„ ì „ë§", "subsections": []},
                ],
            }

    def _extract_keywords_from_query(self, query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
            import re

            # í•œêµ­ì–´, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±° í›„ ë‹¨ì–´ ë¶„ë¦¬
            words = re.findall(r"\b\w+\b", query)

            # ê¸¸ì´ê°€ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ í‚¤ì›Œë“œë¡œ ì¶”ì¶œ
            keywords = [word for word in words if len(word) >= 2]

            return keywords[:10]  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []
